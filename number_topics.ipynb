{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "number_topics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4NpTFwBxH-1",
        "outputId": "f5646170-ba35-4ce0-9164-4667ee900054"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "n5HPAW7WxEyI",
        "outputId": "49d82ca2-bfe0-453a-c30c-5e67bb212e79"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.chdir('..')\n",
        "\n",
        "# Read data into papers\n",
        "papers = pd.read_csv('/content/drive/MyDrive/sentisum/sentisum-assessment-dataset.csv')\n",
        "# Print head\n",
        "papers.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tires where delivered to the garage of my choi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Easy Tyre Selection Process, Competitive Prici...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Very easy to use and good value for money.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Really easy and convenient to arrange</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It was so easy to select tyre sizes and arrang...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  Tires where delivered to the garage of my choi...\n",
              "1  Easy Tyre Selection Process, Competitive Prici...\n",
              "2         Very easy to use and good value for money.\n",
              "3              Really easy and convenient to arrange\n",
              "4  It was so easy to select tyre sizes and arrang..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KSv-Psixr0l",
        "outputId": "b4c50fbf-a19a-4ef8-bea3-a1da7daf1681"
      },
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = papers['text'].map(lambda x: re.sub('[,\\.!?]', ' ', x))\n",
        "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x))\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    tires where delivered to the garage of my choi...\n",
              "1    easy tyre selection process  competitive prici...\n",
              "2           very easy to use and good value for money \n",
              "3                really easy and convenient to arrange\n",
              "4    it was so easy to select tyre sizes and arrang...\n",
              "Name: paper_text_processed, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4HjDtwTUPrA"
      },
      "source": [
        "fg = papers['paper_text_processed']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "f0VgBUChURwu",
        "outputId": "e148897e-cf73-4d05-de42-58fcd56545d3"
      },
      "source": [
        "fg[10129]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i ordered the tyre i needed on line  booked a specified time at a local garage and i had the tyre fitted  all worked very well  to time  and i would use  again  good price for the tyre  too  as i did a quick search on-line '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW5S_aYZx1Os",
        "outputId": "365c06cf-0bcc-43d7-b22c-5617807649bd"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tires', 'where', 'delivered', 'to', 'the', 'garage', 'of', 'my', 'choice', 'the', 'garage', 'notified', 'me', 'when', 'they', 'had', 'been', 'delivered', 'day', 'and', 'time', 'was', 'arranged', 'with', 'the', 'garage', 'and', 'went', 'and', 'had']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKgqsuCmx4_G",
        "outputId": "dcb7d26b-9df6-450f-d0b5-1690d67a80dd"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xr5gFa3x7-h",
        "outputId": "a74a2c8d-3335-41ab-c0c9-b2c6d4ea1d9e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewamO5iZyAZA"
      },
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foQQMT7LyFBk",
        "outputId": "e23d57ad-1c21-493b-c41a-47a3af6ea0c1"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.2.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBU3JxWIyGu7",
        "outputId": "94ba3b08-a1ee-4c0c-c48e-bbeadfe5e663"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tire', 'deliver', 'garage', 'choice', 'garage', 'notify', 'deliver', 'day', 'time', 'arrange', 'garage', 'go', 'fit', 'free', 'experience']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr459el3yOsU",
        "outputId": "c5d48a66-11f0-405e-e9e3-16a37e8fd87b"
      },
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 3), (8, 1), (9, 1), (10, 1), (11, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3QGdixhzE-G"
      },
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=30, \n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)\n",
        "                                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4xsK-ZozIAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406a852f-0649-45ed-dc5b-885205e70a3c"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(26,\n",
            "  '0.137*\"tyre\" + 0.110*\"day\" + 0.069*\"fit\" + 0.067*\"new\" + 0.049*\"garage\" + '\n",
            "  '0.044*\"order\" + 0.040*\"car\" + 0.033*\"get\" + 0.029*\"later\" + 0.023*\"go\"'),\n",
            " (8,\n",
            "  '0.073*\"much\" + 0.041*\"complaint\" + 0.039*\"already\" + 0.036*\"supply\" + '\n",
            "  '0.034*\"agree\" + 0.032*\"else\" + 0.032*\"run\" + 0.028*\"morning\" + 0.025*\"late\" '\n",
            "  '+ 0.025*\"usual\"'),\n",
            " (14,\n",
            "  '0.399*\"use\" + 0.227*\"easy\" + 0.106*\"website\" + 0.078*\"always\" + '\n",
            "  '0.025*\"star\" + 0.020*\"garage\" + 0.017*\"clear\" + 0.013*\"pricing\" + '\n",
            "  '0.011*\"full\" + 0.010*\"premium\"'),\n",
            " (15,\n",
            "  '0.524*\"good\" + 0.204*\"price\" + 0.047*\"tyre\" + 0.047*\"fitting\" + '\n",
            "  '0.024*\"local\" + 0.021*\"garage\" + 0.019*\"experience\" + 0.017*\"centre\" + '\n",
            "  '0.006*\"specify\" + 0.005*\"michelin\"'),\n",
            " (13,\n",
            "  '0.262*\"choice\" + 0.236*\"customer\" + 0.111*\"service\" + 0.055*\"front\" + '\n",
            "  '0.041*\"perfect\" + 0.019*\"satisfied\" + 0.017*\"progress\" + 0.016*\"wide\" + '\n",
            "  '0.014*\"various\" + 0.014*\"manufacturer\"'),\n",
            " (23,\n",
            "  '0.101*\"time\" + 0.095*\"first\" + 0.058*\"know\" + 0.053*\"even\" + 0.050*\"check\" '\n",
            "  '+ 0.050*\"wheel\" + 0.049*\"second\" + 0.049*\"go\" + 0.044*\"tyre\" + '\n",
            "  '0.044*\"wrong\"'),\n",
            " (11,\n",
            "  '0.159*\"helpful\" + 0.112*\"use\" + 0.085*\"start\" + 0.080*\"finish\" + '\n",
            "  '0.079*\"never\" + 0.052*\"let\" + 0.041*\"year\" + 0.040*\"number\" + '\n",
            "  '0.029*\"easily\" + 0.026*\"ever\"'),\n",
            " (5,\n",
            "  '0.186*\"fit\" + 0.156*\"tyre\" + 0.114*\"order\" + 0.109*\"garage\" + 0.085*\"local\" '\n",
            "  '+ 0.057*\"easy\" + 0.056*\"time\" + 0.040*\"deliver\" + 0.032*\"online\" + '\n",
            "  '0.027*\"get\"'),\n",
            " (28,\n",
            "  '0.119*\"say\" + 0.109*\"problem\" + 0.102*\"book\" + 0.075*\"receive\" + '\n",
            "  '0.073*\"slot\" + 0.072*\"could\" + 0.054*\"booking\" + 0.047*\"email\" + '\n",
            "  '0.043*\"provide\" + 0.042*\"however\"'),\n",
            " (29,\n",
            "  '0.229*\"go\" + 0.193*\"really\" + 0.138*\"staff\" + 0.067*\"also\" + '\n",
            "  '0.053*\"reliable\" + 0.034*\"help\" + 0.020*\"move\" + 0.018*\"straight_forward\" + '\n",
            "  '0.013*\"read\" + 0.012*\"sensibly\"'),\n",
            " (2,\n",
            "  '0.242*\"fitter\" + 0.170*\"efficient\" + 0.146*\"friendly\" + 0.131*\"service\" + '\n",
            "  '0.072*\"communication\" + 0.047*\"send\" + 0.038*\"round\" + 0.014*\"suggest\" + '\n",
            "  '0.009*\"proficient\" + 0.009*\"faultless\"'),\n",
            " (25,\n",
            "  '0.177*\"time\" + 0.094*\"appointment\" + 0.072*\"give\" + 0.071*\"arrive\" + '\n",
            "  '0.071*\"tell\" + 0.057*\"book\" + 0.054*\"would\" + 0.045*\"issue\" + '\n",
            "  '0.034*\"garage\" + 0.029*\"minute\"'),\n",
            " (20,\n",
            "  '0.454*\"great\" + 0.191*\"service\" + 0.153*\"price\" + 0.035*\"tyre\" + '\n",
            "  '0.034*\"fantastic\" + 0.014*\"partner\" + 0.014*\"garage\" + 0.010*\"motor\" + '\n",
            "  '0.006*\"stuck\" + 0.005*\"anywhere_else\"'),\n",
            " (17,\n",
            "  '0.241*\"quick\" + 0.178*\"value\" + 0.114*\"easy\" + 0.082*\"money\" + '\n",
            "  '0.058*\"professional\" + 0.058*\"save\" + 0.047*\"size\" + 0.035*\"smoothly\" + '\n",
            "  '0.025*\"organise\" + 0.024*\"arrange\"'),\n",
            " (7,\n",
            "  '0.190*\"process\" + 0.162*\"simple\" + 0.090*\"pick\" + 0.056*\"lot\" + '\n",
            "  '0.053*\"last\" + 0.042*\"many\" + 0.036*\"year\" + 0.031*\"ordering\" + '\n",
            "  '0.030*\"decent\" + 0.018*\"impressed\"'),\n",
            " (18,\n",
            "  '0.146*\"delivery\" + 0.103*\"deal\" + 0.093*\"site\" + 0.074*\"guy\" + 0.060*\"turn\" '\n",
            "  '+ 0.054*\"mobile\" + 0.053*\"nice\" + 0.048*\"budget\" + 0.034*\"feel\" + '\n",
            "  '0.031*\"require\"'),\n",
            " (1,\n",
            "  '0.199*\"work\" + 0.192*\"do\" + 0.081*\"service\" + 0.076*\"brilliant\" + '\n",
            "  '0.052*\"quickly\" + 0.037*\"delay\" + 0.037*\"first_class\" + 0.028*\"busy\" + '\n",
            "  '0.018*\"none\" + 0.015*\"vast\"'),\n",
            " (24,\n",
            "  '0.393*\"service\" + 0.321*\"excellent\" + 0.070*\"price\" + 0.056*\"fast\" + '\n",
            "  '0.027*\"fitting\" + 0.019*\"pleased\" + 0.016*\"locally\" + 0.015*\"tyre\" + '\n",
            "  '0.010*\"class\" + 0.009*\"amazing\"'),\n",
            " (21,\n",
            "  '0.081*\"phone\" + 0.069*\"polite\" + 0.068*\"reasonable\" + 0.066*\"replace\" + '\n",
            "  '0.057*\"friend\" + 0.054*\"prompt\" + 0.049*\"team\" + 0.045*\"family\" + '\n",
            "  '0.031*\"replacement\" + 0.029*\"road\"'),\n",
            " (0,\n",
            "  '0.143*\"car\" + 0.089*\"get\" + 0.089*\"want\" + 0.082*\"back\" + 0.079*\"come\" + '\n",
            "  '0.051*\"able\" + 0.045*\"drive\" + 0.036*\"exactly\" + 0.033*\"ready\" + '\n",
            "  '0.026*\"state\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C2W0BkszLEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415ab5b0-4670-457e-de42-243dddb01b0b"
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coherence Score:  0.45971321723251796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un8c3GLRzOsr"
      },
      "source": [
        "# supporting function\n",
        "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
        "    \n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=dictionary,\n",
        "                                           num_topics=k, \n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=a,\n",
        "                                           eta=b)\n",
        "    \n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "    \n",
        "    return coherence_model_lda.get_coherence()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZupB8pmzbWw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "926061b5-8e83-461f-d01c-33f6fb51dfd1"
      },
      "source": [
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "grid = {}\n",
        "grid['Validation_Set'] = {}\n",
        "\n",
        "# Topics range\n",
        "min_topics = 12\n",
        "max_topics = 30\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Alpha parameter\n",
        "alpha = list(np.arange(0.01, 1, 0.3))\n",
        "alpha.append('symmetric')\n",
        "alpha.append('asymmetric')\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 1, 0.3))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
        "               corpus]\n",
        "\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Alpha': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "# Can take a long time to run\n",
        "if 1 == 1:\n",
        "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
        "    \n",
        "    # iterate through validation corpuses\n",
        "    for i in range(len(corpus_sets)):\n",
        "        # iterate through number of topics\n",
        "        for k in topics_range:\n",
        "            # iterate through alpha values\n",
        "            for a in alpha:\n",
        "                # iterare through beta values\n",
        "                for b in beta:\n",
        "                    # get the coherence score for the given parameters\n",
        "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
        "                                                  k=k, a=a, b=b)\n",
        "                    # Save the model results\n",
        "                    model_results['Validation_Set'].append(corpus_title[i])\n",
        "                    model_results['Topics'].append(k)\n",
        "                    model_results['Alpha'].append(a)\n",
        "                    model_results['Beta'].append(b)\n",
        "                    model_results['Coherence'].append(cv)\n",
        "                    print(cv)\n",
        "                    pbar.update(1)\n",
        "    pd.DataFrame(model_results).to_csv('/content/drive/MyDrive/sentisum/lda_tuning_results.csv', index=False)\n",
        "    pbar.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1080 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
            "  diff = np.log(self.expElogbeta)\n",
            "Process ForkPoolWorker-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 105, in worker\n",
            "    initializer(*initargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/models/ldamulticore.py\", line 333, in worker_e_step\n",
            "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/models/ldamodel.py\", line 725, in do_estep\n",
            "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gensim/models/ldamodel.py\", line 677, in inference\n",
            "    Elogthetad = dirichlet_expectation(gammad)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFull\u001b[0m                                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    287\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                         \u001b[0mjob_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m                         \u001b[0mchunk_put\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFull\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c758500647d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;31m# get the coherence score for the given parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n\u001b[0;32m---> 50\u001b[0;31m                                                   k=k, a=a, b=b)\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0;31m# Save the model results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0mmodel_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Validation_Set'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_title\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-ad0dd2c2e33b>\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[0;34m(corpus, dictionary, k, a, b)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                            \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                            \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                            eta=b)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcoherence_model_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;31m# in case the input job queue is full, keep clearing the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         \u001b[0;31m# result queue, to make sure we don't deadlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                         \u001b[0mprocess_result_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[0;34m(force)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \"\"\"\n\u001b[1;32m    267\u001b[0m                 \u001b[0mmerged_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                     \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0mqueue_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mempty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGztvHgKze1n"
      },
      "source": [
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=10, \n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=30,\n",
        "                                           alpha='symmetric',\n",
        "                                           eta=0.91)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64aReaav2Zxc",
        "outputId": "ad3cd28f-8919-41d1-f6f3-61d53ca63a13"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.086*\"process\" + 0.042*\"communication\" + 0.024*\"whole\" + 0.019*\"perfect\" + '\n",
            "  '0.014*\"ordering\" + 0.008*\"smooth\" + 0.006*\"regard\" + '\n",
            "  '0.004*\"straightforward\" + 0.004*\"simple\" + 0.002*\"stuff\"'),\n",
            " (1,\n",
            "  '0.044*\"money\" + 0.038*\"professional\" + 0.035*\"start\" + 0.032*\"finish\" + '\n",
            "  '0.031*\"save\" + 0.012*\"easily\" + 0.009*\"amazing\" + 0.009*\"value\" + '\n",
            "  '0.005*\"faultless\" + 0.004*\"painless\"'),\n",
            " (2,\n",
            "  '0.119*\"recommend\" + 0.071*\"would\" + 0.056*\"definitely\" + 0.038*\"helpful\" + '\n",
            "  '0.034*\"highly\" + 0.032*\"staff\" + 0.024*\"friendly\" + 0.019*\"service\" + '\n",
            "  '0.013*\"excellent\" + 0.012*\"friend\"'),\n",
            " (3,\n",
            "  '0.019*\"rate\" + 0.012*\"continue\" + 0.010*\"motor\" + 0.006*\"guarantee\" + '\n",
            "  '0.006*\"performance\" + 0.005*\"grip\" + 0.004*\"sport\" + 0.004*\"independent\" + '\n",
            "  '0.004*\"noise\" + 0.004*\"vast\"'),\n",
            " (4,\n",
            "  '0.042*\"deal\" + 0.020*\"smoothly\" + 0.016*\"transaction\" + 0.008*\"go\" + '\n",
            "  '0.007*\"impressed\" + 0.006*\"anywhere_else\" + 0.006*\"tyer\" + 0.005*\"serious\" '\n",
            "  '+ 0.003*\"contract\" + 0.003*\"pleasure\"'),\n",
            " (5,\n",
            "  '0.049*\"tyre\" + 0.026*\"fit\" + 0.025*\"time\" + 0.021*\"garage\" + 0.015*\"car\" + '\n",
            "  '0.015*\"go\" + 0.015*\"day\" + 0.015*\"order\" + 0.013*\"get\" + 0.011*\"would\"'),\n",
            " (6,\n",
            "  '0.026*\"complaint\" + 0.004*\"fashion\" + 0.003*\"sensible\" + 0.002*\"classic\" + '\n",
            "  '0.002*\"mileage\" + 0.002*\"forget\" + 0.002*\"important\" + 0.002*\"fill\" + '\n",
            "  '0.002*\"worked_perfectly\" + 0.001*\"humour\"'),\n",
            " (7,\n",
            "  '0.041*\"delivery\" + 0.027*\"wheel\" + 0.010*\"high\" + 0.010*\"balance\" + '\n",
            "  '0.008*\"torque\" + 0.007*\"standard\" + 0.007*\"valve\" + 0.007*\"damage\" + '\n",
            "  '0.005*\"nut\" + 0.005*\"alloy\"'),\n",
            " (8,\n",
            "  '0.044*\"customer\" + 0.035*\"fault\" + 0.020*\"team\" + 0.006*\"seamless\" + '\n",
            "  '0.004*\"support\" + 0.003*\"welcome\" + 0.003*\"machine\" + 0.003*\"total\" + '\n",
            "  '0.003*\"care\" + 0.002*\"leo\"'),\n",
            " (9,\n",
            "  '0.074*\"service\" + 0.064*\"price\" + 0.063*\"good\" + 0.053*\"use\" + 0.047*\"easy\" '\n",
            "  '+ 0.046*\"great\" + 0.045*\"tyre\" + 0.030*\"excellent\" + 0.024*\"fit\" + '\n",
            "  '0.019*\"fitting\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovy8C9Yl9ymK"
      },
      "source": [
        "topics = lda_model.print_topics()\n",
        "tot = []\n",
        "for topic in topics:\n",
        "    tot.append(topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BTMrNtWl-B4X",
        "outputId": "476813b7-777e-4b66-9261-460239f2bf3e"
      },
      "source": [
        "import pandas as pd\n",
        "ee = pd.DataFrame(tot)\n",
        "ee.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.086*\"process\" + 0.042*\"communication\" + 0.02...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.044*\"money\" + 0.038*\"professional\" + 0.035*\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.119*\"recommend\" + 0.071*\"would\" + 0.056*\"def...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.019*\"rate\" + 0.012*\"continue\" + 0.010*\"motor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.042*\"deal\" + 0.020*\"smoothly\" + 0.016*\"trans...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0                                                  1\n",
              "0  0  0.086*\"process\" + 0.042*\"communication\" + 0.02...\n",
              "1  1  0.044*\"money\" + 0.038*\"professional\" + 0.035*\"...\n",
              "2  2  0.119*\"recommend\" + 0.071*\"would\" + 0.056*\"def...\n",
              "3  3  0.019*\"rate\" + 0.012*\"continue\" + 0.010*\"motor...\n",
              "4  4  0.042*\"deal\" + 0.020*\"smoothly\" + 0.016*\"trans..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iQDdaR8RJ_L"
      },
      "source": [
        "ee.to_csv(\"final.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqx0klxORZVe"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/sentisum/sentisum-assessment-dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X050-u0RwLb",
        "outputId": "9980ea2f-346c-401b-dde5-8042e5743ec8"
      },
      "source": [
        "td = []\n",
        "td = df[\"text\"]\n",
        "td"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Tires where delivered to the garage of my choi...\n",
              "1        Easy Tyre Selection Process, Competitive Prici...\n",
              "2               Very easy to use and good value for money.\n",
              "3                    Really easy and convenient to arrange\n",
              "4        It was so easy to select tyre sizes and arrang...\n",
              "                               ...                        \n",
              "10127    I ordered the wrong tyres, however [REDACTED] ...\n",
              "10128    Good experience, first time I have used [REDAC...\n",
              "10129    I ordered the tyre I needed on line, booked a ...\n",
              "10130    Excellent service from point of order to fitti...\n",
              "10131    Seamless, well managed at both ends. I would r...\n",
              "Name: text, Length: 10132, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIZCaC7Hb23M",
        "outputId": "f5cb0e55-a967-4b43-f22c-a264806a1f21"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQJ9H4hRXFJ6"
      },
      "source": [
        "def prepare_text_for_lda(text):\n",
        "    tokens = tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 3]\n",
        "    tokens = [token for token in tokens if token not in en_stop]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-nAZH0PXzlO"
      },
      "source": [
        "import spacy\n",
        "spacy.load('en')\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "\n",
        "def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    tokens = parser(text)\n",
        "    for token in tokens:\n",
        "        if token.orth_.isspace():\n",
        "            continue\n",
        "        elif token.like_url:\n",
        "            lda_tokens.append('URL')\n",
        "        elif token.orth_.startswith('@'):\n",
        "            lda_tokens.append('SCREEN_NAME')\n",
        "        else:\n",
        "            lda_tokens.append(token.lower_)\n",
        "    return lda_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHe4pw_hYN4z"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "    \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "def get_lemma2(word):\n",
        "    return WordNetLemmatizer().lemmatize(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejZkzt1Db-pW",
        "outputId": "a13fe337-5670-4b46-87fb-57e3d52a70df"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RcBnbnnW6SJ"
      },
      "source": [
        "pred = []\n",
        "for sent in td:\n",
        "  new_doc = sent\n",
        "  new_doc = prepare_text_for_lda(new_doc)\n",
        "  new_doc_bow = id2word.doc2bow(new_doc)\n",
        "  pred.append(lda_model.get_document_topics(new_doc_bow))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzfgOZWkcWZV"
      },
      "source": [
        "rows = zip(td, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq3WedMicdqS"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open(\"Hemlo.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    for row in rows:\n",
        "        writer.writerow(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo-WdUYR4YRN",
        "outputId": "f6c9b130-8b7d-462f-a7e7-9b611cc30666"
      },
      "source": [
        "pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.16)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JaZ4Qd02e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7f6c471-a9ae-466d-d736-76f7e78d118e"
      },
      "source": [
        "num_topics=10\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "import pickle \n",
        "\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./content/drive/MyDrive/sentisum/ldavis_tuned_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './content/drive/MyDrive/sentisum/ldavis_tuned_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  EPS = np.finfo(np.float).eps\n",
            "/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el8501401631716811049877980820\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el8501401631716811049877980820_data = {\"mdsDat\": {\"x\": [-0.25349288101809525, -0.3591073954708738, 0.01936917819386383, 0.08995840511032345, 0.08616596500268361, 0.08566237168265048, 0.08355907279206573, 0.08526886717501767, 0.08169642046406723, 0.08091999606829679], \"y\": [-0.1329474745898319, 0.07333918233807789, 0.18803048449266002, -0.03670749278204173, -0.012439696993916066, -0.015023235219845082, -0.028421306411480596, -0.014525883793985825, -0.010573655027426236, -0.010730922012210454], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [41.22795731747209, 40.26641795518992, 4.973176899089735, 2.991415965422535, 2.3922688786831103, 2.196544174774591, 1.8843298804853006, 1.569550778569968, 1.3291151039155185, 1.1692230463972333]}, \"tinfo\": {\"Term\": [\"price\", \"would\", \"recommend\", \"service\", \"good\", \"easy\", \"great\", \"customer\", \"excellent\", \"use\", \"definitely\", \"process\", \"friendly\", \"tyre\", \"value\", \"helpful\", \"highly\", \"staff\", \"go\", \"delivery\", \"car\", \"day\", \"money\", \"efficient\", \"quick\", \"choice\", \"communication\", \"professional\", \"start\", \"simple\", \"car\", \"day\", \"go\", \"appointment\", \"take\", \"say\", \"tell\", \"arrive\", \"wait\", \"email\", \"new\", \"hour\", \"change\", \"however\", \"call\", \"work\", \"later\", \"slot\", \"turn\", \"back\", \"come\", \"next\", \"pay\", \"know\", \"ask\", \"wrong\", \"phone\", \"week\", \"check\", \"receive\", \"even\", \"still\", \"get\", \"tyre\", \"time\", \"book\", \"fit\", \"give\", \"date\", \"garage\", \"order\", \"make\", \"would\", \"need\", \"could\", \"customer\", \"fitter\", \"fitting\", \"deliver\", \"use\", \"service\", \"well\", \"price\", \"easy\", \"great\", \"quick\", \"choice\", \"competitive\", \"quality\", \"selection\", \"brilliant\", \"always\", \"range\", \"product\", \"reliable\", \"suit\", \"convenient\", \"excellent\", \"first_class\", \"locally\", \"prompt\", \"simple\", \"fast\", \"value\", \"organise\", \"top\", \"ease\", \"good\", \"reasonable\", \"plenty\", \"efficient\", \"navigate\", \"cost_effective\", \"service\", \"thank\", \"use\", \"website\", \"local\", \"cheap\", \"fitting\", \"site\", \"tyre\", \"choose\", \"fit\", \"garage\", \"time\", \"order\", \"find\", \"really\", \"buy\", \"fitter\", \"recommend\", \"definitely\", \"highly\", \"helpful\", \"staff\", \"friend\", \"pleased\", \"courteous\", \"knowledgeable\", \"thoroughly\", \"family\", \"lovely\", \"reservation\", \"swift\", \"hesitation\", \"polite\", \"short_notice\", \"recomend\", \"progress\", \"member\", \"ue\\u00ba\", \"reccomende\", \"bloke\", \"personal\", \"disabled\", \"pleasant\", \"allround\", \"would\", \"fifth\", \"guy\", \"friendly\", \"happily\", \"outstanding\", \"people\", \"man\", \"excellent\", \"service\", \"efficient\", \"fantastic\", \"company\", \"other\", \"really\", \"fast\", \"experience\", \"delivery\", \"wheel\", \"balance\", \"high\", \"torque\", \"damage\", \"valve\", \"standard\", \"alloy\", \"wheel_nut\", \"nut\", \"rim\", \"mark\", \"cover\", \"bolt\", \"cap\", \"dirty\", \"photo\", \"tighten\", \"adjustment\", \"bar\", \"valve_stem\", \"crack\", \"annoyance\", \"balancing\", \"inspection\", \"metal\", \"finger\", \"seal\", \"bead\", \"wash\", \"watch\", \"steering\", \"remove\", \"properly\", \"air\", \"proficient\", \"assist\", \"seat\", \"money\", \"start\", \"finish\", \"professional\", \"save\", \"exactly\", \"amazing\", \"easily\", \"painless\", \"speedy\", \"equally\", \"smooth_transaction\", \"downhill\", \"recovered\", \"convienient\", \"importantly\", \"tho\", \"patient\", \"grace\", \"cash\", \"addition\", \"savibg\", \"decently\", \"quid\", \"saved\", \"same\", \"autumn\", \"deserved\", \"five_star\", \"closest\", \"award\", \"advertise\", \"technical\", \"whatsoever\", \"value\", \"meet\", \"satisfied\", \"detail\", \"especially\", \"promise\", \"process\", \"communication\", \"tire\", \"perfect\", \"whole\", \"ordering\", \"smooth\", \"doubtful\", \"beginning\", \"loved\", \"flexibility\", \"label\", \"walking\", \"slick\", \"worked_perfectly\", \"bam\", \"s\", \"tyresgreat\", \"favourable\", \"driving\", \"remarkably\", \"rummage\", \"burton\", \"trent\", \"deliveryquality\", \"pricessuper\", \"productsvery\", \"offish\", \"custmer\", \"okey\", \"contemplate\", \"unsure\", \"reminder\", \"straightforward\", \"several\", \"privately\", \"publicly\", \"regard\", \"simple\", \"distance\", \"little\", \"yet\", \"clearly\", \"fault\", \"team\", \"class\", \"slow\", \"seamless\", \"support\", \"total\", \"welcome\", \"eden\", \"repeat\", \"execution\", \"machine\", \"turnaround\", \"terrific\", \"professionalism\", \"dealership\", \"comment\", \"mechanical\", \"leo\", \"deflation\", \"count\", \"timing\", \"beforehand\", \"convinient\", \"debate\", \"ground\", \"shortage\", \"swore\", \"made_mistake\", \"damaged\", \"customer\", \"respond\", \"response\", \"stuck\", \"care\", \"resolve\", \"technical\", \"idea\", \"poor\", \"deal\", \"smoothly\", \"system\", \"transaction\", \"impressed\", \"tyer\", \"serious\", \"pleasure\", \"sensible\", \"bmw\", \"tye\", \"exaggerate\", \"mange\", \"procure\", \"sized\", \"hitch\", \"consultation\", \"dead\", \"recieve\", \"worked_perfectly\", \"castle\", \"incorrectly\", \"pensby\", \"commit\", \"contract\", \"dundee\", \"relate\", \"lloyds\", \"angle\", \"humour\", \"run_flat\", \"cold\", \"straight_away\", \"commercial\", \"woulnt\", \"dozen\", \"run\", \"rate\", \"performance\", \"grip\", \"faultless\", \"michelin_pilot\", \"fuel\", \"weather\", \"economy\", \"improvement\", \"increase\", \"noise\", \"expert\", \"meaning\", \"practical\", \"confusing\", \"rain\", \"flap\", \"befor\", \"edge\", \"score\", \"test\", \"mannere\", \"sport\", \"efficiency\", \"ghastly\", \"pat\", \"swan\", \"endure\", \"gilmar\", \"molesey\", \"significantly\", \"park\", \"lower\", \"void\", \"safety\", \"become\", \"condition\", \"info\", \"road\", \"complaint\", \"independent\", \"mileage\", \"seat\", \"marvelous\", \"bloody\", \"chuff\", \"fashion\", \"write\", \"forget\", \"revolutionise\", \"reap\", \"remarkable\", \"reward\", \"rude\", \"director\", \"managing\", \"door\", \"brigade\", \"celtic\", \"texte\", \"important\", \"rescue\", \"cycle\", \"degradation\", \"natural\", \"press\", \"stamp\", \"child\", \"screen\", \"strand\", \"waiting_room\", \"paperwork\", \"ahead\", \"wife\", \"vast\", \"link\", \"point\"], \"Freq\": [2776.0, 878.0, 649.0, 3715.0, 3004.0, 2069.0, 2002.0, 374.0, 1396.0, 2759.0, 308.0, 205.0, 336.0, 4185.0, 417.0, 208.0, 191.0, 181.0, 726.0, 142.0, 700.0, 665.0, 116.0, 450.0, 595.0, 585.0, 104.0, 102.0, 93.0, 324.0, 695.3923437498279, 660.9242232577561, 721.040700156138, 443.99733521508284, 355.9249074714758, 350.35677274751293, 317.9245053705837, 315.98305365596354, 284.33348328872745, 247.4717549389619, 396.02916600084933, 234.2148949343877, 239.32853073733364, 216.90820857210684, 191.04331254731522, 375.9574159708867, 177.63479950622715, 197.80124378395504, 177.4049562558528, 207.12529082203065, 190.7043627812917, 165.58960542807822, 210.6608565035038, 169.96522434353702, 167.42138585022363, 134.12806390845907, 125.73131766510008, 121.66924401734116, 152.1918440643375, 206.85664531196895, 162.58707402287598, 137.31392814656138, 578.2346625216427, 2219.1883744912056, 1122.169633551215, 486.19976056936093, 1185.6373287916497, 272.2686579864725, 244.64278781934226, 945.1870676079585, 660.56197121826, 260.70280255083844, 487.6271817020099, 283.66187997240326, 272.0967566548765, 272.6395071843144, 336.0250587731092, 446.9004972754632, 265.3518571205468, 444.89989041777244, 385.2163672503925, 260.5244585798982, 2769.6366978547776, 2064.7825016032266, 1995.5484976038547, 589.4927913170085, 579.1846457175467, 351.11995815593076, 258.98395561859934, 159.3616746996926, 159.37309496231558, 351.29679857321963, 152.0276290317758, 103.23836713428778, 106.48450017308896, 121.36100619433988, 228.77100731056788, 1323.4812828784375, 76.7200065785404, 74.62329539250105, 91.86093560360023, 303.18263967819354, 240.9045153873778, 389.0437288643016, 59.64215811002729, 138.0667563564423, 50.06019003280826, 2765.0614015132023, 106.41450765118141, 45.01229605224959, 413.42633687108923, 46.85121881459956, 51.26087027362204, 3226.1572629399143, 265.1271579982121, 2309.875148042434, 416.00694848733934, 630.2542623443977, 277.2640982827513, 833.9725435188467, 134.45128303529862, 1962.2647448685254, 371.7616640358971, 1064.5947570137719, 829.4005976657688, 737.9048040189851, 493.57418941490675, 272.56411882348715, 247.90873202145448, 268.3476516280834, 284.2190177903356, 644.1930346238981, 302.14694742421455, 186.60781556591522, 203.7026138497395, 175.7982382793539, 62.92835571901719, 52.908960237800535, 42.710400966140845, 15.018230757838037, 17.7422932349924, 46.46039099580189, 14.97167103270002, 13.236993568054443, 9.466960967646743, 20.48084025907291, 55.262201408068506, 5.208348677899028, 16.535719500035167, 16.281362478920112, 7.931344148210536, 4.391782502638579, 4.391547843137816, 4.438744996839063, 5.33277088553025, 4.111840972486668, 23.69064033314608, 4.046855430097851, 386.0577331316272, 4.446076844627697, 41.2569805757032, 126.91408210203313, 5.190278319327476, 6.134609338494807, 14.450087304624281, 7.791584860547567, 68.40256824943765, 101.28074622647608, 33.44065932358861, 16.033015768889122, 20.150795456079923, 8.639208409975577, 15.124913779580238, 13.214095342576728, 11.40861000438092, 136.0583050226208, 89.69006981861203, 33.117654187654786, 35.07518078052323, 27.172074030635592, 22.733464947844716, 22.899492316863554, 23.340346425490356, 16.87806297877173, 15.944186916559076, 17.396599701402344, 14.855094365103081, 11.364875515982542, 15.57021971958552, 6.756736873370928, 7.113835121409381, 8.630261731367215, 6.978887324284789, 7.557751275515203, 6.954670225071283, 4.970903002810935, 12.330933790010384, 5.280957683981313, 5.182180725942723, 5.011147083311719, 4.431695386301007, 4.722920574671337, 4.85850772082098, 4.413023359284577, 4.38061196835299, 5.732630261096073, 6.177337917911837, 5.016761971663459, 8.47473410491287, 7.68334977888477, 5.96666387685652, 6.106746196833914, 5.372497286943468, 5.203230676009588, 112.04573917542425, 89.1103301798808, 82.02547318179724, 96.94625919520263, 77.97789898717969, 52.40644083213029, 23.758345059317985, 28.387264531350716, 10.603049223627904, 8.627379649776438, 4.476758811345837, 5.776813738197687, 2.8955349527918557, 2.8955349527918557, 3.0491542443719553, 6.198139893750128, 2.849061764847396, 3.15367013944044, 2.294314274332276, 2.2922389038321542, 4.439290780259643, 2.151525931598049, 2.5524672121577385, 2.3367304839030787, 3.5170421920337858, 2.4685422271093582, 2.0825685264859577, 2.0825685264859577, 2.0825685264859577, 1.6771236054609115, 2.0769383690264203, 2.469328740792568, 2.999716458169408, 4.450566267124768, 24.1876140320663, 3.034804314359344, 3.019305564591859, 2.586143167048301, 2.483014291287668, 2.5367812405760017, 200.3713081977326, 98.280521307857, 68.90407978705315, 44.564941851688985, 54.64803222845851, 33.01297969433608, 16.7764702149718, 3.3942770041656574, 3.395055170023885, 3.125182346803469, 4.171955847025812, 2.851550478198449, 2.6401228423794074, 1.9931247382324002, 3.088669567694127, 2.7527362303974634, 3.017953710501116, 1.814609924109828, 1.9983324314436617, 2.5314663149122083, 1.6672950195117302, 2.925061380106725, 1.544680542468639, 1.544680542468639, 1.3312184751792164, 1.3312184751792164, 1.3312184751792164, 1.261372865411258, 1.2401254269589865, 1.2401254269589865, 1.496654224151166, 1.4891358547690459, 3.497384676913038, 10.053333403507049, 4.2692864810799085, 1.6539057188855846, 1.6539057188855846, 4.554854405371399, 9.956332647138645, 2.033901270013199, 2.1593961866285363, 1.9364862162473075, 1.7752902592365367, 66.16701585279627, 38.76228372142997, 19.815962485714145, 15.649779507832413, 11.555423216000218, 7.5025874696536325, 6.1813608719362705, 6.533514219461602, 8.57443170632999, 4.6884166864954935, 4.065572652533211, 6.133638464155098, 3.56861939624194, 3.113208674465537, 4.414298101606139, 3.170457590161637, 4.53496433372764, 3.1298060893541018, 7.440656324792661, 3.046085967598942, 2.0772792148810737, 2.1496862200940767, 2.922480614903657, 2.4302927636705385, 2.4302927636705385, 2.4302927636705385, 2.4302927636705385, 2.4302927636705385, 2.0059695100302917, 2.296820551129199, 96.02100461248997, 4.2951562828390815, 7.050447137957012, 3.820945419069292, 5.361739883276602, 4.177322333290512, 2.743020259758292, 3.1122521516928052, 3.4022738239707784, 69.9072334515261, 32.931243573442224, 39.13526602712322, 26.64970094061233, 15.757503387374632, 9.424049962954133, 8.661536998274258, 4.118248567657412, 3.815305813852294, 2.303183722173897, 1.763215371808329, 1.94878568129272, 1.94878568129272, 1.94878568129272, 1.9471275094825502, 4.592302547956285, 1.9094803002561445, 2.0246152379164686, 2.923969682315309, 2.293417450554292, 1.5559839114494372, 1.734436564725737, 1.7147021501675794, 2.2664562816787894, 4.3134362338239605, 1.3997680608412635, 1.3968839288909778, 1.1553423076766811, 1.3935446905944262, 1.3935446905944262, 2.1268232441350894, 1.8156889742654678, 2.726775447091888, 1.4990149890891125, 1.4990149890891125, 1.66160073587188, 1.5724482694500315, 28.505583529883335, 8.711377992089895, 7.104352130561542, 8.052737874739433, 4.148498363137167, 2.9730634578584993, 3.961519279442433, 4.164223139939664, 2.2665891608777713, 2.2522228575134724, 4.644286545916045, 2.710394914160373, 2.61962956842716, 1.7269324686953271, 1.8319838704167433, 1.865087261751786, 2.0711320093332533, 1.8181302398900667, 2.274722468466807, 1.6236200411008679, 1.9763655901191808, 4.368206575194394, 5.55110482362385, 2.3204445911060385, 1.2641478737921457, 1.2641478737921457, 1.2641478737921457, 1.262674002604078, 1.3094548511296196, 1.3094548511296196, 3.191533904715521, 2.968626250003773, 1.6438519893317887, 1.3691309978091102, 2.121044359982714, 2.016321197003439, 1.99282759754283, 1.8154596544179271, 2.0028170471713995, 33.69824842187727, 5.7851327044271805, 2.5815556619982227, 4.791677218335324, 1.543722725926941, 1.5438470063378729, 1.5437224293149816, 4.752514354905753, 1.975012086825845, 2.320245419983248, 1.1685413730784473, 1.1771126430183125, 1.1771126430183125, 1.1770300365876574, 1.030302477496477, 0.8761120132042143, 0.8761120132042143, 0.8761195768091755, 1.0748629370547693, 1.1565719682272648, 1.3117524490865637, 2.208348558350596, 0.6689427228787952, 0.6351569151997668, 0.6351569151997668, 0.6351569151997668, 0.6351569151997668, 0.6351569151997668, 0.6626691574813749, 0.8317513959534767, 0.8212515550544717, 0.9893066263116396, 1.801520934038241, 0.9573637428885133, 1.5526126312640465, 1.0749224077526018, 0.7800894528664662, 0.8031502911694076], \"Total\": [2776.0, 878.0, 649.0, 3715.0, 3004.0, 2069.0, 2002.0, 374.0, 1396.0, 2759.0, 308.0, 205.0, 336.0, 4185.0, 417.0, 208.0, 191.0, 181.0, 726.0, 142.0, 700.0, 665.0, 116.0, 450.0, 595.0, 585.0, 104.0, 102.0, 93.0, 324.0, 700.3304186111823, 665.7008267623056, 726.5953495033206, 449.20803847667156, 360.5351109620815, 355.0071419885758, 322.1840390677005, 320.5580248699398, 288.75251123360835, 251.58021964398196, 402.74989082198556, 238.3933700886313, 243.94059744884123, 221.44084726437092, 195.0606887626454, 384.0569563953896, 181.65881460542454, 202.45144132160937, 181.74820330071415, 212.21963403198131, 195.56219358053275, 169.96964292770372, 216.4571677538647, 175.0007438471483, 172.4805082848011, 138.2416608638021, 129.70283235017908, 125.53135459019076, 157.09390197997686, 213.52769309484316, 167.97907836547617, 141.77081204638625, 710.5016136557638, 4185.053167454241, 1863.6218188176015, 674.8126714139865, 2253.961523939155, 325.41095477377837, 285.9710957125306, 1778.1584092486178, 1158.0582485770256, 318.11008802204583, 878.3200242972938, 368.69176050658916, 359.9347346049628, 374.71784578614887, 624.3974441018908, 1284.6423931842965, 365.3854928175504, 2759.5434345051917, 3715.6747920949815, 482.9036305490416, 2776.2268633204926, 2069.6985576583447, 2002.7767012379618, 595.1053055742634, 585.618249606932, 355.8589685479079, 264.5449738182613, 163.5765369369018, 163.91095361985842, 362.8881306068264, 157.76290308090947, 107.47701471508742, 110.87680980718557, 126.7284233525578, 241.1467626795015, 1396.2335931090715, 80.98561230126337, 79.7640637646835, 98.28079276345564, 324.9223342349661, 258.35134110996876, 417.57931789396224, 64.03635505315209, 149.34978594763476, 54.26831204803401, 3004.246215999101, 115.69414667341084, 49.014858851685084, 450.57775760338956, 51.194632415609966, 56.02053729544631, 3715.6747920949815, 296.0188147968948, 2759.5434345051917, 478.1495808466374, 818.4446792568878, 338.5275734991398, 1284.6423931842965, 153.34846956860054, 4185.053167454241, 565.3172260821367, 2253.961523939155, 1778.1584092486178, 1863.6218188176015, 1158.0582485770256, 442.21633567813257, 389.7883343679354, 457.385056028271, 624.3974441018908, 649.2395066048981, 308.31403308355044, 191.1175646465627, 208.88464730153126, 181.1181573332871, 67.58757373061493, 58.81008472658166, 48.800293054085785, 19.12342164177764, 22.867031211556586, 61.991947956302226, 21.218625877013647, 19.765863295915285, 14.860638857327931, 32.320230146304176, 94.15209345570689, 9.454416600006901, 30.355173655417744, 30.154228800879807, 14.856067591295925, 8.294373698863636, 8.294310061248018, 8.975119398832456, 10.858684380905643, 8.44734999850823, 52.91200873395432, 9.125848989934171, 878.3200242972938, 10.184658562991217, 96.7871939305148, 336.1168224430035, 12.247091035185997, 15.281523678653139, 74.5967386682472, 24.984325037699005, 1396.2335931090715, 3715.6747920949815, 450.57775760338956, 142.84389435950223, 285.42917678024645, 36.8650020222291, 389.7883343679354, 258.35134110996876, 428.8873870438929, 142.88584139387262, 95.66554360273035, 38.00267294193609, 41.855146397274524, 32.625534015943515, 27.541042687998765, 27.817963946357086, 28.453402219588842, 21.565502723461467, 20.815777132596565, 23.1400954009919, 20.0259269636946, 15.950987591349902, 23.17131839227275, 10.852630147580324, 11.47496972044644, 14.0079648614421, 11.79096428925323, 13.10442987182667, 12.48958696619994, 9.2070380568442, 23.117855762580778, 10.242070049894176, 10.239624611200151, 10.335722033616488, 9.340336076793772, 9.981482973659858, 10.376966212689151, 9.57003906991765, 9.581134839282043, 12.659024680513735, 15.914184711251247, 11.760495269166784, 33.4827330554858, 28.88134789912285, 17.636203629648517, 20.713800582689455, 21.32673154700195, 15.104877550673258, 116.93740166524587, 93.96381588875093, 86.76857795511923, 102.69306164865304, 83.591822163843, 58.19658138004885, 29.384654693411008, 36.00713775419929, 15.24761725252006, 13.261000076067203, 10.748705781350937, 14.339211394397772, 7.319789212721314, 7.319789212721314, 8.207593141872962, 17.66072063828781, 8.133707411449013, 9.041295278057339, 6.959595843371195, 7.045741600789194, 13.933659093906861, 6.767381844385901, 8.137572578127596, 7.461642240877825, 11.517009730377627, 8.18907579801436, 7.627368326128563, 7.627368326128563, 7.627368326128563, 6.203529242968107, 7.685414941023234, 9.260821593898534, 12.723010213779366, 23.938414149454687, 417.57931789396224, 17.402971957231433, 43.436406861310545, 34.60790994161011, 31.158304387154235, 91.28253594020839, 205.52328377577064, 104.66489925238395, 74.25109788755626, 49.48863653610582, 60.772627624762855, 37.90761265361986, 25.12748953964824, 7.572932734950298, 7.591597669525057, 7.301044562104506, 9.834476398856848, 7.8942050701229025, 7.782928960729262, 6.035525298196389, 9.513966472347976, 8.609748714569722, 11.145108225720742, 6.87887799067091, 7.634626400439399, 9.726822843137178, 6.417416700954819, 12.139869165264614, 6.4921827047456215, 6.4921827047456215, 5.706563283568085, 5.706563283568085, 5.706563283568085, 5.578446455755128, 5.566227865018559, 5.566227865018559, 6.736709578500218, 6.802175971435382, 19.93600475962239, 77.62105536643102, 37.3002591925055, 8.262581126892353, 8.262581126892353, 56.87902455643964, 324.9223342349661, 15.070725377561853, 46.19141241159221, 30.561703400728696, 20.143028784003768, 72.34149121368615, 45.33631467638798, 24.78542638879488, 21.27404919419132, 17.329427760359508, 12.0830570642244, 11.421439025019444, 12.168239143979699, 17.433575636435283, 9.790003398254633, 8.761075456278633, 13.608509233322582, 7.928398914125711, 7.160450919653261, 10.422991728514138, 7.62929428460604, 11.103953624083088, 7.699817894944358, 18.828814698462455, 8.407864949994092, 6.219396881750603, 6.576308664659713, 8.962483501188961, 7.890142140186079, 7.890142140186079, 7.890142140186079, 7.890142140186079, 7.890142140186079, 6.527409696542039, 7.73074506463742, 374.71784578614887, 19.858383356121738, 47.49000827665723, 22.24219325586772, 68.42782404611552, 37.191572223894156, 12.723010213779366, 28.931260256332553, 83.59344128477812, 76.03369426764485, 38.19726937009207, 46.04736732973286, 31.719613416948906, 22.869166562219572, 14.23352008601162, 20.000122956800798, 9.594882549197143, 9.879976196270082, 6.782869222938936, 6.0572883436196205, 7.1656642735780745, 7.1656642735780745, 7.1656642735780745, 7.169897620460216, 17.387505925964483, 7.238336219914723, 7.923131427101692, 11.79202038885643, 9.513966472347976, 6.5390371234934, 7.55376682866572, 7.843306634124327, 11.335078106246542, 21.744731981317162, 7.060608933710538, 7.069658935056055, 5.880937222720591, 7.263655325351895, 7.263655325351895, 11.740265455836271, 10.734348675364226, 22.329940837656316, 8.286976714542826, 8.286976714542826, 12.125288145162292, 66.91149616769347, 33.71614973999332, 13.793751965246063, 11.967175028067334, 13.691160783452668, 8.663437721695235, 7.396510515642733, 10.58479816950636, 11.45857510606912, 6.847483632719983, 6.905571875391251, 14.578786492405186, 8.641257245310682, 8.848831456637821, 5.860490803864172, 6.313405249788208, 6.77422998775563, 7.526899721629201, 7.047336864129848, 9.596477436686142, 7.055355686293352, 8.952146434937143, 20.376975331823175, 25.912760268149746, 11.005361818257601, 6.021126989884513, 6.021126989884513, 6.021126989884513, 6.025051578621037, 6.266058546295136, 6.266058546295136, 16.33184566831359, 20.10538992229201, 9.174795346045826, 6.836079676097898, 15.501911930923738, 15.230521954536327, 21.324728890817116, 19.672461152903203, 46.023992489671784, 39.05929403654717, 11.814893727602279, 7.374244510894911, 15.104877550673258, 5.657530289276087, 5.66475563617211, 5.666901808831674, 18.97150828559, 9.096308754514201, 10.78131334333483, 5.6343933178584455, 5.709092299400981, 5.709092299400981, 5.71002775354431, 5.464313337528315, 5.358021324474001, 5.358021324474001, 5.464400757303485, 7.060482425324869, 7.807079420964184, 8.926944969030968, 16.063118188349254, 5.480617651216637, 5.2501826492488926, 5.2501826492488926, 5.2501826492488926, 5.2501826492488926, 5.2501826492488926, 5.493465090797562, 6.996650993119061, 7.836898537294186, 11.275351297674217, 36.630550792129, 11.747612113072456, 31.803285197899733, 32.23394603392923, 12.360262571440018, 69.13495293121542], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.1682, -4.219, -4.132, -4.6168, -4.8379, -4.8537, -4.9508, -4.957, -5.0625, -5.2013, -4.7312, -5.2564, -5.2348, -5.3332, -5.4601, -4.7832, -5.5329, -5.4254, -5.5342, -5.3793, -5.4619, -5.6031, -5.3624, -5.5771, -5.5921, -5.8139, -5.8785, -5.9113, -5.6875, -5.3806, -5.6214, -5.7904, -4.3527, -3.0077, -3.6896, -4.526, -3.6346, -5.1059, -5.2128, -3.8613, -4.2196, -5.1493, -4.5231, -5.0649, -5.1065, -5.1045, -4.8955, -4.6103, -5.1316, -4.6148, -4.7588, -5.1499, -2.7626, -3.0563, -3.0904, -4.3098, -4.3274, -4.8279, -5.1323, -5.6179, -5.6178, -4.8274, -5.665, -6.052, -6.021, -5.8903, -5.2563, -3.501, -6.3489, -6.3766, -6.1688, -4.9747, -5.2046, -4.7254, -6.6007, -5.7613, -6.7758, -2.7642, -6.0217, -6.8821, -4.6646, -6.8421, -6.7521, -2.61, -5.1088, -2.9441, -4.6583, -4.2429, -5.0641, -3.9628, -5.7878, -3.1072, -4.7708, -3.7187, -3.9683, -4.0852, -4.4874, -5.0812, -5.176, -5.0968, -5.0393, -2.1296, -2.8867, -3.3686, -3.2809, -3.4283, -4.4556, -4.629, -4.8431, -5.8883, -5.7216, -4.759, -5.8914, -6.0146, -6.3498, -5.5781, -4.5855, -6.9473, -5.7921, -5.8076, -6.5268, -7.1179, -7.1179, -7.1072, -6.9237, -7.1837, -5.4325, -7.1996, -2.6416, -7.1056, -4.8778, -3.7541, -6.9508, -6.7836, -5.9269, -6.5445, -4.3722, -3.9797, -5.0878, -5.8229, -5.5943, -6.4413, -5.8812, -6.0163, -6.1632, -3.1762, -3.5929, -4.5892, -4.5318, -4.7871, -4.9654, -4.9582, -4.9391, -5.2633, -5.3202, -5.233, -5.3909, -5.6587, -5.3439, -6.1787, -6.1272, -5.934, -6.1464, -6.0667, -6.1499, -6.4857, -5.5772, -6.4252, -6.4441, -6.4776, -6.6005, -6.5368, -6.5085, -6.6047, -6.6121, -6.3431, -6.2684, -6.4765, -5.9522, -6.0502, -6.3031, -6.2799, -6.408, -6.44, -3.1469, -3.3759, -3.4587, -3.2916, -3.5093, -3.9067, -4.6978, -4.5198, -5.5046, -5.7108, -6.3669, -6.1119, -6.8026, -6.8026, -6.7509, -6.0415, -6.8188, -6.7172, -7.0353, -7.0362, -6.3753, -7.0996, -6.9287, -7.017, -6.6082, -6.9621, -7.1322, -7.1322, -7.1322, -7.3487, -7.1349, -6.9618, -6.7673, -6.3727, -4.6799, -6.7556, -6.7607, -6.9156, -6.9563, -6.9349, -2.4802, -3.1926, -3.5477, -3.9835, -3.7795, -4.2835, -4.9604, -6.5583, -6.5581, -6.6409, -6.352, -6.7326, -6.8096, -7.0907, -6.6527, -6.7678, -6.6758, -7.1845, -7.0881, -6.8516, -7.2692, -6.7071, -7.3456, -7.3456, -7.4943, -7.4943, -7.4943, -7.5482, -7.5652, -7.5652, -7.3772, -7.3822, -6.5284, -5.4725, -6.329, -7.2773, -7.2773, -6.2642, -5.4822, -7.0705, -7.0106, -7.1195, -7.2065, -3.4349, -3.9697, -4.6406, -4.8766, -5.1799, -5.6119, -5.8056, -5.7502, -5.4783, -6.082, -6.2245, -5.8133, -6.3549, -6.4914, -6.1423, -6.4732, -6.1153, -6.4861, -5.6201, -6.5132, -6.896, -6.8618, -6.5547, -6.7391, -6.7391, -6.7391, -6.7391, -6.7391, -6.931, -6.7956, -3.0625, -6.1696, -5.674, -6.2866, -5.9478, -6.1974, -6.618, -6.4918, -6.4027, -3.1971, -3.9499, -3.7773, -4.1615, -4.687, -5.2011, -5.2854, -6.0289, -6.1053, -6.61, -6.8772, -6.7771, -6.7771, -6.7771, -6.778, -5.9199, -6.7975, -6.7389, -6.3714, -6.6143, -7.0022, -6.8936, -6.9051, -6.6261, -5.9826, -7.108, -7.1101, -7.2999, -7.1125, -7.1125, -6.6897, -6.8479, -6.4412, -7.0395, -7.0395, -6.9365, -6.9917, -3.9279, -5.1134, -5.3173, -5.192, -5.8553, -6.1884, -5.9014, -5.8515, -6.4598, -6.4661, -5.7424, -6.2809, -6.315, -6.7317, -6.6726, -6.6547, -6.5499, -6.6802, -6.4562, -6.7934, -6.5968, -5.8037, -5.564, -6.4363, -7.0436, -7.0436, -7.0436, -7.0448, -7.0084, -7.0084, -6.1175, -6.1899, -6.781, -6.9639, -6.5261, -6.5768, -6.5885, -6.6817, -6.5835, -3.6324, -5.3946, -6.2015, -5.583, -6.7157, -6.7156, -6.7157, -5.5912, -6.4693, -6.3082, -6.9941, -6.9868, -6.9868, -6.9869, -7.12, -7.2821, -7.2821, -7.2821, -7.0777, -7.0044, -6.8785, -6.3576, -7.5519, -7.6038, -7.6038, -7.6038, -7.6038, -7.6038, -7.5613, -7.3341, -7.3468, -7.1606, -6.5612, -7.1934, -6.7099, -7.0776, -7.3982, -7.3691], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.879, 0.8789, 0.8784, 0.8744, 0.8732, 0.8729, 0.8727, 0.8717, 0.8706, 0.8696, 0.8692, 0.8684, 0.867, 0.8654, 0.8652, 0.8647, 0.8637, 0.8628, 0.8619, 0.8618, 0.8609, 0.8599, 0.8589, 0.8569, 0.8563, 0.8558, 0.855, 0.8548, 0.8544, 0.8543, 0.8534, 0.8541, 0.6801, 0.2517, 0.3788, 0.5582, 0.2436, 0.7078, 0.73, 0.2541, 0.3246, 0.687, 0.2976, 0.6239, 0.6063, 0.568, 0.2665, -0.1698, 0.5662, -0.9389, -1.3805, 0.2689, 0.9073, 0.9073, 0.906, 0.9002, 0.8986, 0.8962, 0.8884, 0.8835, 0.8816, 0.8772, 0.8726, 0.8694, 0.8692, 0.8664, 0.857, 0.8561, 0.8555, 0.843, 0.8421, 0.8404, 0.8397, 0.8389, 0.8386, 0.8311, 0.8289, 0.8267, 0.826, 0.8245, 0.8236, 0.821, 0.8209, 0.7684, 0.7994, 0.7318, 0.7704, 0.6484, 0.71, 0.4776, 0.7781, 0.1522, 0.4905, 0.1596, 0.147, -0.0168, 0.0568, 0.4257, 0.4571, 0.3764, 0.1226, 2.9933, 2.9809, 2.9772, 2.976, 2.9713, 2.9297, 2.8954, 2.8678, 2.7595, 2.7474, 2.7127, 2.6524, 2.6002, 2.5502, 2.5449, 2.4683, 2.4049, 2.3937, 2.3848, 2.3735, 2.3653, 2.3652, 2.297, 2.29, 2.2811, 2.1976, 2.1879, 2.1791, 2.1723, 2.1484, 2.0272, 2.1426, 2.0884, 1.3597, 1.8359, -0.015, -0.6013, 0.4004, 0.814, 0.3504, 1.5502, -0.2481, 0.0281, -0.6257, 3.4605, 3.4449, 3.3718, 3.3327, 3.3265, 3.3176, 3.3149, 3.3113, 3.2643, 3.2428, 3.2241, 3.2107, 3.1704, 3.1119, 3.0356, 3.0313, 3.0251, 2.985, 2.959, 2.9239, 2.8931, 2.8809, 2.847, 2.8284, 2.7855, 2.7639, 2.7611, 2.7506, 2.7353, 2.7268, 2.7172, 2.5631, 2.6575, 2.1355, 2.1853, 2.4257, 2.288, 2.1308, 2.4437, 3.6902, 3.6799, 3.6767, 3.6753, 3.6634, 3.6281, 3.5204, 3.4952, 3.3696, 3.303, 2.857, 2.8238, 2.8055, 2.8055, 2.7427, 2.6858, 2.6839, 2.6797, 2.6232, 2.61, 2.5891, 2.587, 2.5735, 2.5719, 2.5467, 2.5338, 2.4348, 2.4348, 2.4348, 2.4249, 2.4245, 2.4111, 2.288, 2.0505, 0.8843, 1.9864, 1.0667, 1.139, 1.2033, 0.1499, 3.7929, 3.7553, 3.7435, 3.7135, 3.7121, 3.68, 3.4143, 3.0158, 3.0136, 2.9698, 2.9608, 2.8, 2.7372, 2.7103, 2.6933, 2.678, 2.5119, 2.4857, 2.4779, 2.4722, 2.4705, 2.3951, 2.3825, 2.3825, 2.3628, 2.3628, 2.3628, 2.3316, 2.3168, 2.3168, 2.3139, 2.2992, 2.0778, 1.7744, 1.6507, 2.2097, 2.2097, 1.2936, 0.3329, 1.8155, 0.7553, 1.0594, 1.3894, 3.8824, 3.8149, 3.7478, 3.6646, 3.5663, 3.495, 3.3576, 3.3497, 3.262, 3.2353, 3.2038, 3.1747, 3.1733, 3.1387, 3.1124, 3.0935, 3.0761, 3.0714, 3.0432, 2.9563, 2.875, 2.8534, 2.851, 2.794, 2.794, 2.794, 2.794, 2.794, 2.7917, 2.7579, 2.61, 2.4405, 2.0642, 2.2101, 1.4251, 1.7852, 2.4372, 1.742, 0.7701, 4.0704, 4.006, 3.9917, 3.9802, 3.7819, 3.742, 3.3175, 3.3086, 3.2029, 3.0743, 2.9203, 2.8523, 2.8523, 2.8523, 2.8508, 2.823, 2.8218, 2.79, 2.7599, 2.7317, 2.7187, 2.683, 2.634, 2.5447, 2.5367, 2.5362, 2.5328, 2.5271, 2.5033, 2.5033, 2.446, 2.3774, 2.0516, 2.4445, 2.4445, 2.1669, 0.4036, 4.1528, 3.8611, 3.7992, 3.7899, 3.5843, 3.4092, 3.3379, 3.3084, 3.2151, 3.2002, 3.1767, 3.1612, 3.1034, 3.0988, 3.0834, 3.0308, 3.0303, 2.9658, 2.8811, 2.8515, 2.81, 2.7806, 2.7799, 2.764, 2.7598, 2.7598, 2.7598, 2.758, 2.7551, 2.7551, 2.688, 2.4078, 2.6012, 2.7126, 2.3316, 2.2986, 1.9503, 1.9378, 1.186, 4.3012, 3.7348, 3.3992, 3.3007, 3.15, 3.1488, 3.1484, 3.0646, 2.9215, 2.9127, 2.8757, 2.8698, 2.8698, 2.8696, 2.7804, 2.638, 2.638, 2.6183, 2.5665, 2.5393, 2.5311, 2.4645, 2.3456, 2.3367, 2.3367, 2.3367, 2.3367, 2.3367, 2.3338, 2.3192, 2.1931, 2.0155, 1.4366, 1.9416, 1.4292, 1.0481, 1.686, -0.0064]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 1, 2, 4, 6, 1, 2, 5, 1, 2, 3, 10, 1, 2, 3, 4, 1, 2, 4, 7, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 8, 9, 1, 2, 4, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 6, 1, 2, 3, 4, 1, 2, 4, 7, 1, 2, 4, 6, 7, 9, 1, 2, 9, 1, 2, 4, 7, 1, 2, 6, 1, 2, 3, 1, 2, 10, 1, 2, 8, 1, 2, 4, 1, 2, 3, 1, 2, 10, 1, 2, 3, 4, 5, 1, 2, 6, 1, 2, 3, 4, 6, 1, 2, 3, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 7, 8, 1, 2, 4, 5, 1, 2, 8, 1, 2, 4, 10, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 10, 1, 2, 3, 4, 1, 2, 3, 4, 6, 1, 2, 10, 1, 2, 7, 1, 2, 3, 4, 6, 7, 1, 2, 5, 1, 2, 3, 6, 7, 8, 1, 2, 3, 4, 1, 2, 4, 7, 1, 2, 3, 8, 1, 2, 3, 8, 1, 2, 3, 6, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 9, 1, 2, 8, 1, 2, 3, 6, 1, 2, 8, 1, 2, 3, 5, 1, 2, 5, 1, 2, 7, 1, 2, 3, 5, 1, 2, 3, 4, 6, 1, 2, 7, 1, 2, 3, 1, 2, 4, 7, 10, 1, 2, 4, 1, 2, 6, 1, 2, 3, 4, 5, 7, 1, 2, 10, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 1, 2, 3, 1, 2, 3, 8, 1, 2, 3, 4, 8, 1, 2, 7, 1, 2, 7, 1, 2, 5, 1, 2, 3, 6, 1, 2, 7, 8, 1, 2, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 6, 1, 2, 3, 5, 1, 2, 4, 5, 6, 1, 2, 10, 1, 2, 3, 4, 1, 2, 3, 1, 2, 5, 6, 1, 2, 10, 1, 2, 6, 1, 2, 5, 1, 2, 3, 4, 7, 8, 1, 2, 3, 5, 6, 1, 2, 3, 8, 1, 2, 3, 1, 2, 3, 5, 6, 1, 2, 3, 6, 1, 2, 9, 1, 2, 7, 1, 2, 3, 4, 9, 1, 2, 3, 4, 9, 1, 2, 3, 5, 1, 2, 3, 4, 1, 2, 9, 1, 2, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 8, 1, 2, 3, 1, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 9, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 10, 1, 2, 3, 4, 1, 2, 3, 4, 7, 1, 2, 3, 5, 9, 1, 2, 3, 6, 1, 2, 3, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 9, 1, 2, 6, 1, 2, 4, 7, 10, 1, 2, 3, 1, 2, 3, 5, 1, 2, 9, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 9, 1, 2, 9, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 1, 2, 3, 9, 1, 2, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 6, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 6, 8, 1, 2, 3, 1, 2, 3, 4, 1, 2, 8, 9, 1, 2, 7, 1, 2, 5, 6, 9, 10, 1, 2, 3, 5, 1, 2, 3, 5, 8, 1, 2, 3, 9, 1, 2, 8, 1, 2, 3, 9, 1, 2, 3, 6, 10, 1, 2, 3, 9, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 6, 1, 2, 4, 1, 2, 7, 1, 2, 3, 4, 5, 10, 1, 2, 3, 4, 5, 6, 1, 2, 8, 1, 2, 3, 4, 1, 2, 3, 1, 2, 6, 1, 2, 3, 4, 1, 2, 9, 1, 2, 3, 4, 6, 7, 8, 1, 2, 3, 7, 1, 2, 3, 5, 1, 2, 3, 4, 1, 2, 10, 1, 2, 8, 1, 2, 9, 1, 2, 3, 4, 1, 2, 10, 1, 2, 3, 9, 1, 2, 7, 1, 2, 3, 5, 1, 2, 3, 4, 5, 7, 1, 2, 4, 1, 2, 9, 1, 2, 10, 1, 2, 9, 1, 2, 3, 5, 1, 2, 10, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 1, 2, 6, 1, 2, 6, 1, 2, 3, 5, 6, 1, 2, 3, 6, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 1, 2, 3, 4, 5, 10, 1, 2, 4, 8, 9, 1, 2, 9, 1, 2, 3, 5, 1, 2, 3, 4, 1, 2, 4, 8, 1, 2, 3, 4, 5, 1, 2, 3, 6, 1, 2, 3, 4, 9, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 5, 8, 9, 1, 2, 3, 6, 1, 2, 3, 4, 8, 1, 2, 3, 5, 1, 2, 3, 4, 10, 1, 2, 3, 4, 5, 6, 1, 2, 4, 7, 1, 2, 9, 1, 2, 10, 1, 2, 3, 4, 1, 2, 6, 1, 2, 5, 6, 1, 2, 3, 6, 1, 2, 8, 1, 2, 3, 5, 1, 2, 6, 1, 2, 3, 5, 1, 2, 3, 4, 7, 1, 2, 3, 4, 1, 2, 3, 5, 6, 1, 2, 3, 5, 8, 1, 2, 3, 4, 1, 2, 3, 4, 7, 8, 10, 1, 2, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 9, 1, 2, 3, 4, 7, 1, 2, 3, 9, 1, 2, 3, 5, 1, 2, 10, 1, 2, 3, 6, 1, 2, 3, 1, 2, 3, 4, 1, 2, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 5, 1, 2, 3, 5, 6, 7, 1, 2, 3, 8, 1, 2, 3, 1, 2, 10, 1, 2, 4, 6, 1, 2, 6, 1, 2, 4, 1, 2, 3, 7, 1, 2, 10, 1, 2, 3, 6, 1, 2, 3, 6, 7, 1, 2, 4, 7, 1, 2, 3, 4, 6, 7, 8, 1, 2, 10, 1, 2, 10, 1, 2, 3, 4, 1, 2, 3, 4, 6, 9, 1, 2, 10, 1, 2, 5, 6, 1, 2, 3, 4, 5, 8, 1, 2, 3, 6, 8, 1, 2, 6, 1, 2, 4, 5, 7, 9, 1, 2, 5, 6, 1, 2, 3, 5, 1, 2, 3, 5, 1, 2, 5, 6, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 9, 1, 2, 6, 8, 10, 1, 2, 4, 1, 2, 3, 5, 6, 7, 1, 2, 3, 4, 6, 10, 1, 2, 3, 1, 2, 6, 8, 1, 2, 3, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 1, 2, 7, 1, 2, 3, 4, 9, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 8, 1, 2, 6, 1, 2, 3, 1, 2, 7, 1, 2, 3, 4, 6, 7, 1, 2, 3, 5, 1, 2, 3, 8, 1, 2, 3, 5, 1, 2, 3, 5, 6, 9, 1, 2, 3, 4, 1, 2, 10, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 8, 1, 2, 3, 5, 6, 1, 2, 3, 6, 7, 10, 1, 2, 7, 1, 2, 3, 1, 2, 3, 5, 7, 1, 2, 9, 1, 2, 3, 4, 5, 1, 2, 7, 1, 2, 3, 8, 1, 2, 3, 4, 1, 2, 4, 7, 1, 2, 5, 6, 7, 9, 1, 2, 3, 4, 1, 2, 7, 1, 2, 3, 6, 9, 1, 2, 3, 10, 1, 2, 3, 4, 5, 1, 2, 5, 1, 2, 3, 1, 2, 4, 7, 1, 2, 3, 1, 2, 3, 7, 1, 2, 3, 6, 1, 2, 3, 5, 7, 1, 2, 4, 1, 2, 3, 5, 7, 1, 2, 3, 8, 1, 2, 6, 1, 2, 3, 4, 1, 2, 7, 1, 2, 8, 1, 2, 3, 8, 1, 2, 3, 4, 1, 2, 6, 1, 2, 3, 1, 2, 3, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 4, 1, 2, 9, 10, 1, 2, 4, 9, 1, 2, 3, 1, 2, 3, 4, 5, 6, 10, 1, 2, 6, 1, 2, 4, 1, 2, 3, 4, 5, 7, 1, 2, 3, 9, 1, 2, 3, 1, 2, 3, 1, 2, 3, 7, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 10, 1, 2, 3, 4, 1, 2, 6, 8, 1, 2, 3, 4, 1, 2, 3, 8, 1, 2, 3, 10, 1, 2, 3, 1, 2, 3, 6], \"Freq\": [0.2153059709428293, 0.1435373139618862, 0.0717686569809431, 0.0717686569809431, 0.2870746279237724, 0.0717686569809431, 0.0717686569809431, 0.1601333979588371, 0.08006669897941855, 0.5604668928559299, 0.08006669897941855, 0.3239453400091998, 0.10798178000306659, 0.21596356000613318, 0.5958657753272745, 0.08512368218961064, 0.08512368218961064, 0.08512368218961064, 0.4536123628415736, 0.0567015453551967, 0.0567015453551967, 0.3402092721311802, 0.04637035420983177, 0.04637035420983177, 0.7882960215671402, 0.04637035420983177, 0.10957884588086017, 0.21915769176172034, 0.4383153835234407, 0.022045361435829528, 0.9672402329970206, 0.002755670179478691, 0.002755670179478691, 0.03403136808765129, 0.03403136808765129, 0.03403136808765129, 0.8167528341036309, 0.13767173072072414, 0.2753434614414483, 0.13767173072072414, 0.13767173072072414, 0.1953196602356292, 0.0976598301178146, 0.4882991505890729, 0.9884061770258324, 0.004452280076692939, 0.0022261400383464694, 0.9857809678238779, 0.0031195600247591075, 0.0031195600247591075, 0.968225347088196, 0.005797756569390395, 0.005797756569390395, 0.005797756569390395, 0.005797756569390395, 0.42200559331676846, 0.1875580414741193, 0.04688951036852983, 0.23444755184264912, 0.04688951036852983, 0.26221363837232486, 0.13110681918616243, 0.13110681918616243, 0.26221363837232486, 0.26023318393967165, 0.13011659196983583, 0.13011659196983583, 0.26023318393967165, 0.9754045658603165, 0.0047120993519822055, 0.0047120993519822055, 0.0047120993519822055, 0.026313938536057454, 0.026313938536057454, 0.026313938536057454, 0.868359971689896, 0.09675182795624179, 0.09675182795624179, 0.09675182795624179, 0.483759139781209, 0.34844222513989215, 0.11614740837996404, 0.34844222513989215, 0.10861256289221417, 0.10861256289221417, 0.10861256289221417, 0.5430628144610709, 0.2087435396274904, 0.1043717698137452, 0.4174870792549808, 0.1043717698137452, 0.45960342139916543, 0.0656576316284522, 0.1313152632569044, 0.0656576316284522, 0.0656576316284522, 0.1313152632569044, 0.1418975734067556, 0.2837951468135112, 0.2837951468135112, 0.22315243311016197, 0.11157621655508099, 0.11157621655508099, 0.33472864966524296, 0.13172457808378057, 0.13172457808378057, 0.3951737342513417, 0.1114191305499609, 0.1114191305499609, 0.4456765221998436, 0.1765301213726737, 0.1765301213726737, 0.3530602427453474, 0.14743023448220222, 0.14743023448220222, 0.29486046896440443, 0.09214356210443213, 0.09214356210443213, 0.6450049347310248, 0.7201998726278318, 0.2741501572760265, 0.0014818927420325756, 0.28326676273937124, 0.14163338136968562, 0.14163338136968562, 0.0061008735408812015, 0.970038893000111, 0.0061008735408812015, 0.0061008735408812015, 0.0061008735408812015, 0.15403140137584626, 0.3080628027516925, 0.3080628027516925, 0.4044732060257018, 0.5859395633237193, 0.0021863416541929824, 0.0021863416541929824, 0.0021863416541929824, 0.9791824339983412, 0.005126609602085556, 0.005126609602085556, 0.08714619945516461, 0.08714619945516461, 0.6100233961861523, 0.9923887090014554, 0.0014278974230236769, 0.0014278974230236769, 0.0014278974230236769, 0.8329944842552063, 0.029227876640533553, 0.014613938320266777, 0.014613938320266777, 0.014613938320266777, 0.07306969160133389, 0.014613938320266777, 0.14192970118120568, 0.14192970118120568, 0.14192970118120568, 0.28385940236241136, 0.1529277141442137, 0.1529277141442137, 0.3058554282884274, 0.256177744859293, 0.256177744859293, 0.1280888724296465, 0.1280888724296465, 0.97974671907624, 0.0040993586572227615, 0.0040993586572227615, 0.0040993586572227615, 0.17133020923670012, 0.8182494475614815, 0.0029539691247706915, 0.0029539691247706915, 0.9675741584124243, 0.006365619463239634, 0.006365619463239634, 0.1820344688592198, 0.1820344688592198, 0.1820344688592198, 0.005122791173283288, 0.9886986964436745, 0.0017075970577610958, 0.0017075970577610958, 0.33609448152990534, 0.6580376164690778, 0.001768918323841607, 0.001768918323841607, 0.001768918323841607, 0.17646326577276034, 0.17646326577276034, 0.35292653154552067, 0.04034628996546475, 0.0806925799309295, 0.806925799309295, 0.5460946374030631, 0.1985798681465684, 0.0496449670366421, 0.0496449670366421, 0.0992899340732842, 0.0496449670366421, 0.16119856308141547, 0.16119856308141547, 0.32239712616283095, 0.2794766679123368, 0.18631777860822452, 0.09315888930411226, 0.09315888930411226, 0.18631777860822452, 0.18631777860822452, 0.9766713928852816, 0.010226925579950592, 0.005113462789975296, 0.005113462789975296, 0.18011602603078689, 0.09005801301539344, 0.09005801301539344, 0.4502900650769672, 0.24134253888878407, 0.12067126944439203, 0.12067126944439203, 0.12067126944439203, 0.44110856168204055, 0.08822171233640812, 0.08822171233640812, 0.17644342467281623, 0.019108602925010182, 0.019108602925010182, 0.009554301462505091, 0.936321543325499, 0.399398552334295, 0.5185174188199619, 0.07006992146215701, 0.003503496073107851, 0.0028101020021513773, 0.9863458027551334, 0.0028101020021513773, 0.025602101232662208, 0.025602101232662208, 0.025602101232662208, 0.8704714419105151, 0.4689391387435745, 0.23446956937178726, 0.04689391387435745, 0.04689391387435745, 0.04689391387435745, 0.04689391387435745, 0.0937878277487149, 0.158393127074101, 0.158393127074101, 0.316786254148202, 0.27630659024893467, 0.13815329512446733, 0.27630659024893467, 0.14844042011124195, 0.2968808402224839, 0.14844042011124195, 0.14844042011124195, 0.18395260072355715, 0.5058696519897822, 0.18395260072355715, 0.03732167042176527, 0.9496291696204718, 0.00414685226908503, 0.00414685226908503, 0.1218383980193006, 0.1218383980193006, 0.36551519405790184, 0.2534808580714406, 0.1267404290357203, 0.2534808580714406, 0.017850596375506133, 0.9103804151508127, 0.017850596375506133, 0.017850596375506133, 0.7556925571479693, 0.23337564264863758, 0.0027782814601028283, 0.0027782814601028283, 0.0027782814601028283, 0.1607872948153978, 0.1607872948153978, 0.3215745896307956, 0.04098336044382731, 0.020491680221913656, 0.8811422495422871, 0.1726272080113462, 0.04315680200283655, 0.6905088320453848, 0.04315680200283655, 0.04315680200283655, 0.09763651245583234, 0.09763651245583234, 0.4881825622791617, 0.17965488015404232, 0.17965488015404232, 0.17965488015404232, 0.7285481678281233, 0.00800602382228707, 0.002668674607429023, 0.002668674607429023, 0.002668674607429023, 0.2561927623131862, 0.19046956397661005, 0.19046956397661005, 0.19046956397661005, 0.03630944591781045, 0.03630944591781045, 0.03630944591781045, 0.8351172561096403, 0.2587072763721775, 0.12935363818608875, 0.12935363818608875, 0.12935363818608875, 0.2587072763721775, 0.8567299411486105, 0.1328805623006008, 0.00349685690264739, 0.9929385294815263, 0.0030043525854206543, 0.0015021762927103271, 0.12621272399690628, 0.25242544799381256, 0.12621272399690628, 0.25242544799381256, 0.02630412765371936, 0.01315206382685968, 0.01315206382685968, 0.01315206382685968, 0.9206444678801776, 0.13107372224685887, 0.13107372224685887, 0.39322116674057667, 0.2534808580714406, 0.1267404290357203, 0.2534808580714406, 0.12288676879980512, 0.24577353759961024, 0.3686603063994154, 0.00324344626807502, 0.00648689253615004, 0.9795207729586561, 0.00324344626807502, 0.11893625860399942, 0.11893625860399942, 0.35680877581199827, 0.11893625860399942, 0.19046956397661005, 0.19046956397661005, 0.19046956397661005, 0.7252614162553073, 0.26547304670477284, 0.002736835533038895, 0.002736835533038895, 0.002736835533038895, 0.013997188108280727, 0.013997188108280727, 0.0069985940541403635, 0.9518087913630895, 0.1752368194845883, 0.1752368194845883, 0.1752368194845883, 0.26221363837232486, 0.13110681918616243, 0.13110681918616243, 0.26221363837232486, 0.6645879522573082, 0.1733707701540804, 0.028895128359013398, 0.0866853850770402, 0.028895128359013398, 0.1866360619791244, 0.1866360619791244, 0.1866360619791244, 0.14277591497285516, 0.07138795748642758, 0.07138795748642758, 0.6424916173778482, 0.11838032047643299, 0.11838032047643299, 0.47352128190573195, 0.4644766475820729, 0.26541522718975596, 0.06635380679743899, 0.13270761359487798, 0.1830026830780013, 0.1830026830780013, 0.1830026830780013, 0.13204923838618554, 0.13204923838618554, 0.3961477151585567, 0.13661595586141545, 0.13661595586141545, 0.40984786758424635, 0.2474168006635727, 0.08247226688785757, 0.08247226688785757, 0.2474168006635727, 0.08247226688785757, 0.16494453377571514, 0.20561698637403608, 0.20561698637403608, 0.10280849318701804, 0.10280849318701804, 0.3084254795610541, 0.1416308436550774, 0.1416308436550774, 0.1416308436550774, 0.1416308436550774, 0.01842695971665526, 0.921347985832763, 0.01842695971665526, 0.08331681402946638, 0.02777227134315546, 0.02777227134315546, 0.7776235976083529, 0.02777227134315546, 0.0004831621475986335, 0.9977298347911782, 0.0004831621475986335, 0.0004831621475986335, 0.26181265752763866, 0.17454177168509244, 0.3490835433701849, 0.057360579427553064, 0.34416347656531837, 0.5162452148479776, 0.31261470886508547, 0.10420490295502849, 0.10420490295502849, 0.10420490295502849, 0.20840980591005698, 0.27259439985181405, 0.09086479995060469, 0.09086479995060469, 0.18172959990120938, 0.18172959990120938, 0.002219372756700135, 0.9166009485171559, 0.07323930097110447, 0.002219372756700135, 0.9817941980873395, 0.0039748752959001595, 0.0039748752959001595, 0.0039748752959001595, 0.16597368287241643, 0.16597368287241643, 0.16597368287241643, 0.18606891291694028, 0.18606891291694028, 0.37213782583388055, 0.09303445645847014, 0.5776951074212157, 0.25675338107609585, 0.03209417263451198, 0.03209417263451198, 0.06418834526902396, 0.970358937470516, 0.005953122315770037, 0.005953122315770037, 0.005953122315770037, 0.005953122315770037, 0.005953122315770037, 0.034366279815289065, 0.017183139907644533, 0.017183139907644533, 0.017183139907644533, 0.8935232751975156, 0.2791088060564869, 0.13955440302824346, 0.2791088060564869, 0.0014324250683200423, 0.947549182693708, 0.04870245232288144, 0.11414123813799014, 0.11414123813799014, 0.11414123813799014, 0.45656495255196056, 0.40336928813039435, 0.5619190661238442, 0.025647758204822763, 0.002331614382256615, 0.002331614382256615, 0.002331614382256615, 0.002331614382256615, 0.23144780246940716, 0.11572390123470358, 0.11572390123470358, 0.3471717037041107, 0.17744239957992347, 0.032262254469077, 0.7420318527887709, 0.0161311272345385, 0.014001298473188515, 0.8540792068644993, 0.11201038778550812, 0.0070006492365942575, 0.2635531094698359, 0.31626373136380304, 0.2635531094698359, 0.0038706979251729302, 0.9328381999666762, 0.05031907302724809, 0.0038706979251729302, 0.027646651547343602, 0.013823325773671801, 0.013823325773671801, 0.013823325773671801, 0.9123395010623389, 0.0730398259005631, 0.0730398259005631, 0.0730398259005631, 0.0730398259005631, 0.5843186072045048, 0.13098217876678897, 0.26196435753357794, 0.13098217876678897, 0.26196435753357794, 0.19637378981633757, 0.09818689490816879, 0.39274757963267515, 0.3753818812356656, 0.6173449010682934, 0.002261336633949793, 0.002261336633949793, 0.002261336633949793, 0.19273455834850478, 0.09636727917425239, 0.09636727917425239, 0.481836395871262, 0.09636727917425239, 0.011524909403462241, 0.011524909403462241, 0.011524909403462241, 0.9450425710839037, 0.012347872314406148, 0.9507861682092734, 0.012347872314406148, 0.5261846697042445, 0.4725014108221083, 0.00044366329654658054, 0.00044366329654658054, 0.5381187946457556, 0.45483850499819817, 0.0016015440316837963, 0.0016015440316837963, 0.0016015440316837963, 0.34795675619267286, 0.6492079075272688, 0.0007784267476346148, 0.0007784267476346148, 0.26221363837232486, 0.13110681918616243, 0.13110681918616243, 0.26221363837232486, 0.2657136502367404, 0.1328568251183702, 0.2657136502367404, 0.10168309520944492, 0.20336619041888984, 0.4067323808377797, 0.18550615646807353, 0.09275307823403676, 0.18550615646807353, 0.09275307823403676, 0.18550615646807353, 0.014795619147178132, 0.014795619147178132, 0.9321240062722222, 0.005950312113102125, 0.6099069915929678, 0.3778448191819849, 0.0029751560565510625, 0.13519888843328484, 0.13519888843328484, 0.4055966652998545, 0.5314487140655377, 0.46621268143950345, 0.0005623795916037435, 0.0005623795916037435, 0.8135097639342442, 0.18156186772926905, 0.0014074563389865817, 0.0014074563389865817, 0.16608186501962158, 0.16608186501962158, 0.16608186501962158, 0.1595899547716896, 0.1595899547716896, 0.1595899547716896, 0.8358661440549566, 0.15057882742166498, 0.0030730372943196934, 0.0030730372943196934, 0.9922992219711488, 0.002752563722527459, 0.0013762818612637294, 0.0013762818612637294, 0.0013762818612637294, 0.07855547882300158, 0.9203639785830482, 0.00033286219840254907, 0.00033286219840254907, 0.14368650457662277, 0.14368650457662277, 0.28737300915324554, 0.001997227148452201, 0.9966163470776482, 0.0004993067871130502, 0.08356190977859353, 0.08356190977859353, 0.08356190977859353, 0.5849333684501546, 0.2534808580714406, 0.1267404290357203, 0.2534808580714406, 0.5165972683937491, 0.020663890735749965, 0.42360976008287426, 0.010331945367874983, 0.010331945367874983, 0.010331945367874983, 0.08165204268727907, 0.24495612806183725, 0.4082602134363954, 0.08165204268727907, 0.004787331251570969, 0.004787331251570969, 0.9766155753204776, 0.1856422424233913, 0.12376149494892755, 0.6188074747446377, 0.047783849111808, 0.023891924555904, 0.023891924555904, 0.83621735945664, 0.023891924555904, 0.0052323814498647404, 0.0052323814498647404, 0.9784553311247065, 0.40258792893045187, 0.11502512255155768, 0.05751256127577884, 0.2875628063788942, 0.9815709216787452, 0.004194747528541646, 0.004194747528541646, 0.9799456725385939, 0.004515878675293059, 0.004515878675293059, 0.004515878675293059, 0.13767173072072414, 0.2753434614414483, 0.13767173072072414, 0.13767173072072414, 0.72585845946353, 0.10369406563764715, 0.10369406563764715, 0.43578089371696044, 0.12450882677627441, 0.062254413388137204, 0.062254413388137204, 0.062254413388137204, 0.12450882677627441, 0.05662283099773606, 0.4529826479818885, 0.05662283099773606, 0.3397369859864164, 0.13118099393075716, 0.04372699797691905, 0.04372699797691905, 0.04372699797691905, 0.6996319676307048, 0.1460390493263255, 0.1460390493263255, 0.1460390493263255, 0.292078098652651, 0.26476856452733205, 0.13238428226366603, 0.26476856452733205, 0.14481059904156637, 0.14481059904156637, 0.14481059904156637, 0.28962119808313275, 0.08463893311742386, 0.16927786623484772, 0.08463893311742386, 0.08463893311742386, 0.5078335987045431, 0.1524974418138459, 0.6099897672553836, 0.0508324806046153, 0.1016649612092306, 0.21412505755216185, 0.10706252877608093, 0.10706252877608093, 0.4282501151043237, 0.9714244423354216, 0.00571426142550248, 0.00571426142550248, 0.00571426142550248, 0.0522918972729947, 0.0522918972729947, 0.7843784590949205, 0.12667519922742915, 0.2533503984548583, 0.12667519922742915, 0.38002559768228744, 0.9798588655696574, 0.005504825087469986, 0.005504825087469986, 0.05311008770412187, 0.424880701632975, 0.3717706139288531, 0.1618088603248007, 0.48542658097440217, 0.08090443016240036, 0.08090443016240036, 0.08090443016240036, 0.08090443016240036, 0.7360675550909838, 0.12989427442782067, 0.02164904573797011, 0.02164904573797011, 0.02164904573797011, 0.04329809147594022, 0.1700409241126686, 0.1700409241126686, 0.1700409241126686, 0.22603849067474172, 0.7697526979734448, 0.0012218296793229282, 0.0012218296793229282, 0.025073948161672327, 0.9402730560627123, 0.012536974080836163, 0.13696670270859881, 0.13696670270859881, 0.41090010812579647, 0.09425681057728721, 0.04712840528864361, 0.7069260793296541, 0.09425681057728721, 0.32698277038876367, 0.21798851359250912, 0.21798851359250912, 0.1469668694571397, 0.07348343472856984, 0.07348343472856984, 0.1469668694571397, 0.07348343472856984, 0.44090060837141903, 0.07348343472856984, 0.15320012784393786, 0.15320012784393786, 0.15320012784393786, 0.3064002556878757, 0.820470679263438, 0.16975255433036648, 0.0031435658209327125, 0.0031435658209327125, 0.4402760524209492, 0.08005019134926349, 0.32020076539705394, 0.04002509567463174, 0.1866360619791244, 0.1866360619791244, 0.1866360619791244, 0.2791088060564869, 0.13955440302824346, 0.2791088060564869, 0.2944499810347057, 0.34352497787382336, 0.19629998735647047, 0.06269204300191998, 0.06269204300191998, 0.06269204300191998, 0.6896124730211197, 0.1767555715778511, 0.1767555715778511, 0.3535111431557022, 0.22601854378181532, 0.11300927189090766, 0.11300927189090766, 0.33902781567272294, 0.1298732013722808, 0.1298732013722808, 0.38961960411684243, 0.4022301488046299, 0.11492289965846568, 0.1723843494876985, 0.1723843494876985, 0.13462512792899428, 0.06731256396449714, 0.5385005117159771, 0.06731256396449714, 0.06731256396449714, 0.06731256396449714, 0.20037102755951208, 0.10018551377975604, 0.5009275688987802, 0.11542762031933013, 0.11542762031933013, 0.4617104812773205, 0.1356071118231261, 0.1356071118231261, 0.40682133546937826, 0.1595899547716896, 0.1595899547716896, 0.1595899547716896, 0.00855158388812741, 0.00855158388812741, 0.00855158388812741, 0.95777739547027, 0.19046956397661005, 0.19046956397661005, 0.19046956397661005, 0.01953329778563047, 0.9180649959246321, 0.01953329778563047, 0.7702911494679969, 0.21969571516516814, 0.0027122927798168906, 0.0027122927798168906, 0.9832404900018483, 0.007448791590923093, 0.002482930530307698, 0.002482930530307698, 0.002482930530307698, 0.002482930530307698, 0.9766449887207671, 0.005883403546510645, 0.005883403546510645, 0.13718562934177678, 0.13718562934177678, 0.06859281467088839, 0.20577844401266518, 0.06859281467088839, 0.06859281467088839, 0.34296407335444196, 0.08643006717743566, 0.04321503358871783, 0.04321503358871783, 0.7346555710082031, 0.17926137822266408, 0.17926137822266408, 0.17926137822266408, 0.17965488015404232, 0.17965488015404232, 0.17965488015404232, 0.570783033420132, 0.4265761248253332, 0.0008635144227233466, 0.0008635144227233466, 0.0008635144227233466, 0.026379925561060318, 0.026379925561060318, 0.026379925561060318, 0.8705375435149905, 0.015616129293585964, 0.9369677576151578, 0.015616129293585964, 0.015616129293585964, 0.6510239707983285, 0.02712599878326369, 0.2441339890493732, 0.02712599878326369, 0.26175400333853005, 0.13087700166926503, 0.3926310050077951, 0.06543850083463251, 0.06543850083463251, 0.06543850083463251, 0.06558401771494653, 0.06558401771494653, 0.06558401771494653, 0.7214241948644119, 0.7916888873598751, 0.027299616805512936, 0.027299616805512936, 0.05459923361102587, 0.027299616805512936, 0.05459923361102587, 0.5968548755522969, 0.04973790629602474, 0.04973790629602474, 0.04973790629602474, 0.14921371888807422, 0.16608186501962158, 0.16608186501962158, 0.16608186501962158, 0.22120724282215135, 0.11060362141107567, 0.11060362141107567, 0.331810864233227, 0.974788694638793, 0.009239703266718417, 0.004619851633359208, 0.004619851633359208, 0.2549944931769573, 0.12749724658847866, 0.12749724658847866, 0.2549944931769573, 0.7507030602108202, 0.026810823578957865, 0.18767576505270506, 0.013405411789478933, 0.013405411789478933, 0.020206658942208317, 0.020206658942208317, 0.020206658942208317, 0.9092996523993743, 0.0724965913929395, 0.0724965913929395, 0.0724965913929395, 0.0724965913929395, 0.6524693225364555, 0.18418437536658513, 0.09209218768329257, 0.4604609384164628, 0.09209218768329257, 0.09209218768329257, 0.971451414875953, 0.007709931864094866, 0.007709931864094866, 0.08481070550874631, 0.08481070550874631, 0.08481070550874631, 0.5936749385612242, 0.3779860277193699, 0.07559720554387397, 0.45358323326324385, 0.018899301385968494, 0.018899301385968494, 0.018899301385968494, 0.017003886402292643, 0.034007772804585286, 0.90120597932151, 0.017003886402292643, 0.20844444835516523, 0.10422222417758262, 0.10422222417758262, 0.10422222417758262, 0.41688889671033047, 0.020401976531767996, 0.9180889439295599, 0.020401976531767996, 0.020401976531767996, 0.8967967340873984, 0.04339339035906766, 0.014464463453022554, 0.014464463453022554, 0.014464463453022554, 0.35049672066532, 0.02124222549486788, 0.5841612011088666, 0.01062111274743394, 0.01062111274743394, 0.01062111274743394, 0.9211248970799489, 0.011962661001038297, 0.011962661001038297, 0.035887983003114896, 0.17063417271137774, 0.17063417271137774, 0.34126834542275547, 0.19046956397661005, 0.19046956397661005, 0.19046956397661005, 0.001080603332399091, 0.9977570769151607, 0.000360201110799697, 0.000360201110799697, 0.1752368194845883, 0.1752368194845883, 0.1752368194845883, 0.3630826679856555, 0.12102755599521851, 0.12102755599521851, 0.24205511199043703, 0.00486562875810712, 0.00486562875810712, 0.00486562875810712, 0.9731257516214239, 0.2791088060564869, 0.13955440302824346, 0.2791088060564869, 0.00930431499842935, 0.9583444448382232, 0.00930431499842935, 0.00930431499842935, 0.1752368194845883, 0.1752368194845883, 0.1752368194845883, 0.009737756221752655, 0.01947551244350531, 0.009737756221752655, 0.9445623535100074, 0.09594174360364346, 0.09594174360364346, 0.09594174360364346, 0.09594174360364346, 0.38376697441457386, 0.04827699272318481, 0.28966195633910885, 0.28966195633910885, 0.28966195633910885, 0.29846560027883745, 0.06632568895085277, 0.5306055116068221, 0.03316284447542638, 0.03316284447542638, 0.7558948630129664, 0.18623496624957142, 0.010954998014680672, 0.032864994044042015, 0.010954998014680672, 0.020349856200424062, 0.9360933852195069, 0.010174928100212031, 0.020349856200424062, 0.5539907644160171, 0.03462442277600107, 0.03462442277600107, 0.27699538220800857, 0.03462442277600107, 0.03462442277600107, 0.03462442277600107, 0.3630826679856555, 0.12102755599521851, 0.12102755599521851, 0.24205511199043703, 0.007560151195214058, 0.9790395797802205, 0.003780075597607029, 0.0016803748691755019, 0.9897407979443706, 0.0016803748691755019, 0.0016803748691755019, 0.0016803748691755019, 0.13401875454730391, 0.13401875454730391, 0.13401875454730391, 0.13401875454730391, 0.26803750909460783, 0.14761825355907499, 0.14761825355907499, 0.14761825355907499, 0.14761825355907499, 0.29523650711814997, 0.012677251501731622, 0.9634711141316034, 0.006338625750865811, 0.006338625750865811, 0.006338625750865811, 0.029659377114873323, 0.029659377114873323, 0.029659377114873323, 0.8601219363313264, 0.31812137272161634, 0.6362427454432327, 0.038482424119550364, 0.0025654949413033575, 0.17515919301303354, 0.17515919301303354, 0.17515919301303354, 0.051860877775754724, 0.9162088407050002, 0.008643479629292454, 0.008643479629292454, 0.12056457892406464, 0.12056457892406464, 0.48225831569625854, 0.969429290410852, 0.009366466574017894, 0.009366466574017894, 0.004683233287008947, 0.2544093294508741, 0.16960621963391606, 0.08480310981695803, 0.2544093294508741, 0.16471656715782312, 0.19765988058938774, 0.5600363283365987, 0.032943313431564626, 0.032943313431564626, 0.0015402636312589048, 0.0015402636312589048, 0.9919297785307346, 0.13661595586141545, 0.13661595586141545, 0.40984786758424635, 0.42194816432172455, 0.42194816432172455, 0.017581173513405192, 0.017581173513405192, 0.08790586756702595, 0.035162347026810384, 0.14144953938885751, 0.14144953938885751, 0.14144953938885751, 0.14144953938885751, 0.00901901851017356, 0.9560159620783975, 0.00901901851017356, 0.17515919301303354, 0.17515919301303354, 0.17515919301303354, 0.1558259415897388, 0.1558259415897388, 0.1558259415897388, 0.3116518831794776, 0.5517655183489409, 0.1504815050042566, 0.1504815050042566, 0.6271889443791795, 0.029866140208532357, 0.23892912166825886, 0.10214501050921805, 0.10214501050921805, 0.10214501050921805, 0.5107250525460902, 0.18246118661060237, 0.18246118661060237, 0.18246118661060237, 0.15177682629323686, 0.05059227543107895, 0.6576995806040263, 0.05059227543107895, 0.7528587345390867, 0.05377562389564905, 0.026887811947824525, 0.026887811947824525, 0.1075512477912981, 0.5539222303616691, 0.05035656639651537, 0.05035656639651537, 0.20142626558606147, 0.7369971341361833, 0.021057060975319522, 0.021057060975319522, 0.021057060975319522, 0.021057060975319522, 0.14739942682723664, 0.021057060975319522, 0.1774813974790255, 0.1774813974790255, 0.1774813974790255, 0.17513049728686228, 0.17513049728686228, 0.17513049728686228, 0.04993526650790846, 0.04993526650790846, 0.04993526650790846, 0.749028997618627, 0.8256563141176324, 0.06518339321981308, 0.021727797739937695, 0.021727797739937695, 0.021727797739937695, 0.04345559547987539, 0.18300561081153743, 0.18300561081153743, 0.18300561081153743, 0.4118660532443238, 0.08237321064886476, 0.08237321064886476, 0.24711963194659428, 0.9116520103977634, 0.0149451149245535, 0.0149451149245535, 0.0149451149245535, 0.0149451149245535, 0.029890229849107, 0.3407077987330803, 0.17035389936654016, 0.08517694968327008, 0.08517694968327008, 0.17035389936654016, 0.17945092676483743, 0.35890185352967485, 0.26917639014725614, 0.32254085962298823, 0.06450817192459765, 0.2580326876983906, 0.06450817192459765, 0.06450817192459765, 0.1290163438491953, 0.12211390206480618, 0.24422780412961237, 0.24422780412961237, 0.12211390206480618, 0.5525318905090457, 0.2992881073590664, 0.02302216210454357, 0.06906648631363071, 0.02392578542049158, 0.01196289271024579, 0.01196289271024579, 0.9331056313991717, 0.2604842810966032, 0.17365618739773547, 0.34731237479547095, 0.08682809369886774, 0.1477676334799376, 0.1477676334799376, 0.2955352669598752, 0.9858956584351282, 0.0028168447383860803, 0.0028168447383860803, 0.0028168447383860803, 0.0028168447383860803, 0.2834725971201507, 0.14173629856007536, 0.2834725971201507, 0.28585104530252026, 0.14292552265126013, 0.14292552265126013, 0.14292552265126013, 0.14292552265126013, 0.2089855626908334, 0.1044927813454167, 0.4179711253816668, 0.05770530994032399, 0.05770530994032399, 0.05770530994032399, 0.05770530994032399, 0.05770530994032399, 0.6924637192838878, 0.06620378064272542, 0.06620378064272542, 0.06620378064272542, 0.3310189032136271, 0.06620378064272542, 0.3310189032136271, 0.006113346196989983, 0.9720220453214072, 0.006113346196989983, 0.10121481875406976, 0.20242963750813953, 0.10121481875406976, 0.40485927501627905, 0.19999877043955117, 0.19999877043955117, 0.04999969260988779, 0.4499972334889901, 0.10361509592257084, 0.8682137647953598, 0.027182142047219884, 0.0002691301192794048, 0.0002691301192794048, 0.6434271643029806, 0.1340473258964543, 0.02680946517929086, 0.02680946517929086, 0.02680946517929086, 0.10723786071716344, 0.02680946517929086, 0.10577067230137395, 0.10577067230137395, 0.5288533615068698, 0.2534808580714406, 0.1267404290357203, 0.2534808580714406, 0.3061503336209455, 0.18369020017256732, 0.061230066724189104, 0.12246013344837821, 0.18369020017256732, 0.02462126839891171, 0.932530540608781, 0.003077658549863964, 0.003077658549863964, 0.003077658549863964, 0.03077658549863964, 0.10433752645208103, 0.8738267840361785, 0.006521095403255064, 0.2789440109009012, 0.1394720054504506, 0.2789440109009012, 0.16568566124622697, 0.16568566124622697, 0.33137132249245393, 0.978012301159477, 0.004939456066462005, 0.004939456066462005, 0.0940112520067915, 0.04700562600339575, 0.752090016054332, 0.07959410337607478, 0.03979705168803739, 0.03979705168803739, 0.07959410337607478, 0.6765498786966357, 0.03979705168803739, 0.0697388421507401, 0.2789553686029604, 0.0697388421507401, 0.41843305290444055, 0.05235976374703821, 0.026179881873519106, 0.026179881873519106, 0.8639361018261305, 0.07540909390421847, 0.07540909390421847, 0.07540909390421847, 0.6786818451379663, 0.46309230957342695, 0.1543641031911423, 0.038591025797785575, 0.038591025797785575, 0.038591025797785575, 0.23154615478671348, 0.01104251517046782, 0.00552125758523391, 0.9717413350011681, 0.00552125758523391, 0.19046956397661005, 0.19046956397661005, 0.19046956397661005, 0.035145182016635836, 0.035145182016635836, 0.035145182016635836, 0.8083391863826243, 0.010642394527527027, 0.010642394527527027, 0.010642394527527027, 0.9471731129499054, 0.25509129771645633, 0.0850304325721521, 0.4251521628607605, 0.9663484184260348, 0.007053638090700984, 0.007053638090700984, 0.007053638090700984, 0.5821779867001403, 0.0895658441077139, 0.13434876616157085, 0.13434876616157085, 0.012883102339683886, 0.8116354474000849, 0.012883102339683886, 0.012883102339683886, 0.12883102339683886, 0.3828044966670958, 0.12760149888903194, 0.12760149888903194, 0.12760149888903194, 0.12760149888903194, 0.12760149888903194, 0.49455554465601487, 0.2247979748436431, 0.1798383798749145, 0.015781779233818845, 0.9547976436460403, 0.007890889616909423, 0.08276051289708852, 0.08276051289708852, 0.08276051289708852, 0.08276051289708852, 0.6620841031767082, 0.16608186501962158, 0.16608186501962158, 0.16608186501962158, 0.06729185801503344, 0.13458371603006689, 0.605626722135301, 0.06729185801503344, 0.06729185801503344, 0.2534808580714406, 0.1267404290357203, 0.2534808580714406, 0.06515030443581724, 0.02171676814527241, 0.02171676814527241, 0.846953957665624, 0.9874211669704522, 0.002773654963400147, 0.002773654963400147, 0.002773654963400147, 0.044114745856077234, 0.022057372928038617, 0.022057372928038617, 0.8602375441935061, 0.07859775188398205, 0.23579325565194614, 0.23579325565194614, 0.07859775188398205, 0.23579325565194614, 0.07859775188398205, 0.9870135122776169, 0.003103816076344707, 0.003103816076344707, 0.003103816076344707, 0.13965600926825766, 0.13965600926825766, 0.418968027804773, 0.33511516168815486, 0.11170505389605162, 0.11170505389605162, 0.11170505389605162, 0.22341010779210324, 0.33606121807712386, 0.2240408120514159, 0.11202040602570795, 0.11202040602570795, 0.08783225491203722, 0.8952133673726871, 0.006756327300925941, 0.0033781636504629704, 0.0033781636504629704, 0.24589033005844277, 0.12294516502922138, 0.36883549508766417, 0.0437310812561719, 0.0874621625123438, 0.7871594626110942, 0.15262014597825563, 0.07631007298912781, 0.6104805839130225, 0.07631007298912781, 0.6020534792363974, 0.39600309062073197, 0.0005365895536866287, 0.15206098907338078, 0.15206098907338078, 0.15206098907338078, 0.30412197814676156, 0.026935628656006447, 0.013467814328003223, 0.013467814328003223, 0.9292791886322224, 0.04686983617408297, 0.9240053417176357, 0.006695690882011853, 0.013391381764023706, 0.006695690882011853, 0.06130167858777839, 0.030650839293889193, 0.8275726609350083, 0.0875546415656934, 0.0875546415656934, 0.0875546415656934, 0.0875546415656934, 0.5253278493941603, 0.03152623541955479, 0.03152623541955479, 0.03152623541955479, 0.8512083563279794, 0.15403140137584626, 0.3080628027516925, 0.3080628027516925, 0.9738748267411593, 0.00550211766520429, 0.00550211766520429, 0.00550211766520429, 0.12612887051108643, 0.12612887051108643, 0.5045154820443457, 0.16509037431796345, 0.16509037431796345, 0.3301807486359269, 0.0702566894174532, 0.0702566894174532, 0.0702566894174532, 0.6323102047570788, 0.5302202651226563, 0.4688112483869543, 0.0002389455904112917, 0.0002389455904112917, 0.14537254496390162, 0.29074508992780324, 0.29074508992780324, 0.12056365390638286, 0.12056365390638286, 0.48225461562553146, 0.14701178037724036, 0.2940235607544807, 0.14701178037724036, 0.14701178037724036, 0.16125855981672274, 0.8370949959025383, 0.0007247575722099899, 0.00036237878610499496, 0.00036237878610499496, 0.002394754618220662, 0.9315595464878376, 0.002394754618220662, 0.002394754618220662, 0.05747411083729589, 0.07189598792552576, 0.03594799396276288, 0.8268038611435462, 0.3027962485746797, 0.04325660693923995, 0.5190792832708795, 0.06204639040764087, 0.8066030752993313, 0.031023195203820436, 0.031023195203820436, 0.1462826718501341, 0.1462826718501341, 0.1462826718501341, 0.1462826718501341, 0.9835412297773458, 0.0034631733442864287, 0.0034631733442864287, 0.26606709811505513, 0.08868903270501838, 0.26606709811505513, 0.08868903270501838, 0.08868903270501838, 0.08868903270501838, 0.08868903270501838, 0.12848633272200646, 0.2569726654440129, 0.3854589981660194, 0.23698508184583558, 0.0789950272819452, 0.47397016369167116, 0.2513480943307149, 0.06283702358267873, 0.06283702358267873, 0.37702214149607244, 0.06283702358267873, 0.12567404716535746, 0.283425338108257, 0.09447511270275234, 0.09447511270275234, 0.37790045081100937, 0.12339234909613729, 0.8700206309151375, 0.0020913957473921575, 0.971868744651731, 0.007966137251243696, 0.007966137251243696, 0.16436231868351392, 0.08218115934175696, 0.08218115934175696, 0.5752681153922987, 0.5404805089231856, 0.4514358273764538, 0.004141613095196824, 0.002070806547598412, 0.002070806547598412, 0.1253215848497775, 0.5430602010157025, 0.0417738616165925, 0.16709544646637, 0.03135925315449081, 0.01045308438483027, 0.01045308438483027, 0.9407775946347243, 0.04804048360193313, 0.04804048360193313, 0.04804048360193313, 0.7686477376309301, 0.03290955283929611, 0.016454776419648057, 0.016454776419648057, 0.016454776419648057, 0.016454776419648057, 0.9050127030806432, 0.8175256058678184, 0.03144329253337763, 0.03144329253337763, 0.06288658506675526, 0.97902145434102, 0.010415121854691702, 0.005207560927345851, 0.0026037804636729256, 0.10510863191566487, 0.10510863191566487, 0.3153258957469946, 0.21021726383132974, 0.5556061418392776, 0.002277074351800318, 0.4394753498974614, 0.001138537175900159, 0.24134253888878407, 0.12067126944439203, 0.12067126944439203, 0.12067126944439203, 0.3298041085633992, 0.1099347028544664, 0.1099347028544664, 0.2198694057089328, 0.9693170579889006, 0.00723370938797687, 0.00723370938797687, 0.7198551635533327, 0.09816206775727264, 0.03272068925242421, 0.06544137850484842], \"Term\": [\"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"addition\", \"adjustment\", \"adjustment\", \"adjustment\", \"adjustment\", \"advertise\", \"advertise\", \"advertise\", \"ahead\", \"ahead\", \"ahead\", \"ahead\", \"air\", \"air\", \"air\", \"air\", \"alloy\", \"alloy\", \"alloy\", \"alloy\", \"allround\", \"allround\", \"allround\", \"always\", \"always\", \"always\", \"always\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"angle\", \"angle\", \"angle\", \"angle\", \"annoyance\", \"annoyance\", \"annoyance\", \"appointment\", \"appointment\", \"appointment\", \"arrive\", \"arrive\", \"arrive\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"assist\", \"assist\", \"assist\", \"assist\", \"assist\", \"autumn\", \"autumn\", \"autumn\", \"autumn\", \"award\", \"award\", \"award\", \"award\", \"back\", \"back\", \"back\", \"back\", \"balance\", \"balance\", \"balance\", \"balance\", \"balancing\", \"balancing\", \"balancing\", \"balancing\", \"bam\", \"bam\", \"bam\", \"bar\", \"bar\", \"bar\", \"bar\", \"bead\", \"bead\", \"bead\", \"bead\", \"become\", \"become\", \"become\", \"become\", \"become\", \"become\", \"befor\", \"befor\", \"befor\", \"beforehand\", \"beforehand\", \"beforehand\", \"beforehand\", \"beginning\", \"beginning\", \"beginning\", \"bloke\", \"bloke\", \"bloke\", \"bloody\", \"bloody\", \"bloody\", \"bmw\", \"bmw\", \"bmw\", \"bolt\", \"bolt\", \"bolt\", \"book\", \"book\", \"book\", \"brigade\", \"brigade\", \"brigade\", \"brilliant\", \"brilliant\", \"brilliant\", \"brilliant\", \"brilliant\", \"burton\", \"burton\", \"burton\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"call\", \"call\", \"call\", \"cap\", \"cap\", \"cap\", \"car\", \"car\", \"car\", \"car\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"cash\", \"cash\", \"cash\", \"cash\", \"castle\", \"castle\", \"castle\", \"celtic\", \"celtic\", \"celtic\", \"celtic\", \"change\", \"change\", \"change\", \"change\", \"cheap\", \"cheap\", \"cheap\", \"cheap\", \"check\", \"check\", \"check\", \"child\", \"child\", \"child\", \"choice\", \"choice\", \"choice\", \"choice\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"chuff\", \"chuff\", \"chuff\", \"class\", \"class\", \"class\", \"clearly\", \"clearly\", \"clearly\", \"clearly\", \"clearly\", \"clearly\", \"closest\", \"closest\", \"closest\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"come\", \"come\", \"come\", \"come\", \"comment\", \"comment\", \"comment\", \"comment\", \"commercial\", \"commercial\", \"commercial\", \"commercial\", \"commit\", \"commit\", \"commit\", \"commit\", \"communication\", \"communication\", \"communication\", \"communication\", \"company\", \"company\", \"company\", \"company\", \"competitive\", \"competitive\", \"competitive\", \"complaint\", \"complaint\", \"complaint\", \"complaint\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"confusing\", \"confusing\", \"confusing\", \"consultation\", \"consultation\", \"consultation\", \"contemplate\", \"contemplate\", \"contemplate\", \"contemplate\", \"contract\", \"contract\", \"contract\", \"convenient\", \"convenient\", \"convenient\", \"convenient\", \"convienient\", \"convienient\", \"convienient\", \"convinient\", \"convinient\", \"convinient\", \"cost_effective\", \"cost_effective\", \"cost_effective\", \"cost_effective\", \"could\", \"could\", \"could\", \"could\", \"could\", \"count\", \"count\", \"count\", \"courteous\", \"courteous\", \"courteous\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"crack\", \"crack\", \"crack\", \"custmer\", \"custmer\", \"custmer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"cycle\", \"cycle\", \"cycle\", \"damage\", \"damage\", \"damage\", \"damage\", \"damaged\", \"damaged\", \"damaged\", \"damaged\", \"damaged\", \"date\", \"date\", \"date\", \"day\", \"day\", \"day\", \"dead\", \"dead\", \"dead\", \"dead\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"dealership\", \"dealership\", \"dealership\", \"debate\", \"debate\", \"debate\", \"decently\", \"decently\", \"decently\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"deflation\", \"deflation\", \"deflation\", \"deflation\", \"degradation\", \"degradation\", \"degradation\", \"deliver\", \"deliver\", \"deliver\", \"deliver\", \"deliver\", \"delivery\", \"delivery\", \"delivery\", \"delivery\", \"deliveryquality\", \"deliveryquality\", \"deliveryquality\", \"deserved\", \"deserved\", \"deserved\", \"deserved\", \"detail\", \"detail\", \"detail\", \"detail\", \"detail\", \"director\", \"director\", \"director\", \"dirty\", \"dirty\", \"dirty\", \"dirty\", \"disabled\", \"disabled\", \"disabled\", \"distance\", \"distance\", \"distance\", \"distance\", \"door\", \"door\", \"door\", \"doubtful\", \"doubtful\", \"doubtful\", \"downhill\", \"downhill\", \"downhill\", \"dozen\", \"dozen\", \"dozen\", \"dozen\", \"dozen\", \"dozen\", \"driving\", \"driving\", \"driving\", \"driving\", \"driving\", \"dundee\", \"dundee\", \"dundee\", \"dundee\", \"ease\", \"ease\", \"ease\", \"easily\", \"easily\", \"easily\", \"easily\", \"easily\", \"easy\", \"easy\", \"easy\", \"easy\", \"economy\", \"economy\", \"economy\", \"eden\", \"eden\", \"eden\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"efficiency\", \"efficiency\", \"efficiency\", \"efficiency\", \"efficiency\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"email\", \"email\", \"email\", \"email\", \"endure\", \"endure\", \"endure\", \"equally\", \"equally\", \"equally\", \"equally\", \"especially\", \"especially\", \"especially\", \"especially\", \"especially\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"exactly\", \"exaggerate\", \"exaggerate\", \"exaggerate\", \"excellent\", \"excellent\", \"excellent\", \"execution\", \"execution\", \"execution\", \"execution\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"expert\", \"expert\", \"expert\", \"expert\", \"family\", \"family\", \"family\", \"family\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"fashion\", \"fashion\", \"fashion\", \"fast\", \"fast\", \"fast\", \"fast\", \"fault\", \"fault\", \"fault\", \"fault\", \"fault\", \"faultless\", \"faultless\", \"faultless\", \"faultless\", \"faultless\", \"favourable\", \"favourable\", \"favourable\", \"favourable\", \"fifth\", \"fifth\", \"fifth\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finger\", \"finger\", \"finger\", \"finger\", \"finger\", \"finish\", \"finish\", \"finish\", \"finish\", \"first_class\", \"first_class\", \"first_class\", \"fit\", \"fit\", \"fit\", \"fit\", \"fitter\", \"fitter\", \"fitter\", \"fitter\", \"fitter\", \"fitting\", \"fitting\", \"fitting\", \"fitting\", \"five_star\", \"five_star\", \"five_star\", \"five_star\", \"flap\", \"flap\", \"flap\", \"flexibility\", \"flexibility\", \"flexibility\", \"forget\", \"forget\", \"forget\", \"forget\", \"forget\", \"friend\", \"friend\", \"friend\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"fuel\", \"fuel\", \"fuel\", \"garage\", \"garage\", \"garage\", \"garage\", \"get\", \"get\", \"get\", \"get\", \"ghastly\", \"ghastly\", \"ghastly\", \"gilmar\", \"gilmar\", \"gilmar\", \"give\", \"give\", \"give\", \"give\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"grace\", \"grace\", \"grace\", \"great\", \"great\", \"great\", \"grip\", \"grip\", \"grip\", \"grip\", \"ground\", \"ground\", \"ground\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"happily\", \"happily\", \"happily\", \"happily\", \"helpful\", \"helpful\", \"helpful\", \"hesitation\", \"hesitation\", \"hesitation\", \"high\", \"high\", \"high\", \"high\", \"high\", \"highly\", \"highly\", \"highly\", \"hitch\", \"hitch\", \"hitch\", \"hitch\", \"hour\", \"hour\", \"hour\", \"however\", \"however\", \"however\", \"however\", \"humour\", \"humour\", \"humour\", \"humour\", \"idea\", \"idea\", \"idea\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"importantly\", \"importantly\", \"importantly\", \"importantly\", \"impressed\", \"impressed\", \"impressed\", \"impressed\", \"impressed\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"incorrectly\", \"incorrectly\", \"incorrectly\", \"increase\", \"increase\", \"increase\", \"increase\", \"independent\", \"independent\", \"independent\", \"independent\", \"independent\", \"info\", \"info\", \"info\", \"info\", \"inspection\", \"inspection\", \"inspection\", \"inspection\", \"know\", \"know\", \"know\", \"know\", \"knowledgeable\", \"knowledgeable\", \"knowledgeable\", \"label\", \"label\", \"label\", \"label\", \"later\", \"later\", \"later\", \"leo\", \"leo\", \"leo\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"lloyds\", \"lloyds\", \"lloyds\", \"local\", \"local\", \"local\", \"local\", \"locally\", \"locally\", \"locally\", \"loved\", \"loved\", \"loved\", \"lovely\", \"lovely\", \"lovely\", \"lovely\", \"lower\", \"lower\", \"lower\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"machine\", \"made_mistake\", \"made_mistake\", \"made_mistake\", \"made_mistake\", \"make\", \"make\", \"make\", \"make\", \"man\", \"man\", \"man\", \"man\", \"managing\", \"managing\", \"managing\", \"mange\", \"mange\", \"mange\", \"mannere\", \"mannere\", \"mannere\", \"mark\", \"mark\", \"mark\", \"mark\", \"marvelous\", \"marvelous\", \"marvelous\", \"meaning\", \"meaning\", \"meaning\", \"meaning\", \"mechanical\", \"mechanical\", \"mechanical\", \"meet\", \"meet\", \"meet\", \"meet\", \"member\", \"member\", \"member\", \"member\", \"member\", \"member\", \"metal\", \"metal\", \"metal\", \"michelin_pilot\", \"michelin_pilot\", \"michelin_pilot\", \"mileage\", \"mileage\", \"mileage\", \"molesey\", \"molesey\", \"molesey\", \"money\", \"money\", \"money\", \"money\", \"natural\", \"natural\", \"natural\", \"navigate\", \"navigate\", \"navigate\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"next\", \"next\", \"next\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"nut\", \"nut\", \"nut\", \"nut\", \"offish\", \"offish\", \"offish\", \"okey\", \"okey\", \"okey\", \"order\", \"order\", \"order\", \"order\", \"order\", \"ordering\", \"ordering\", \"ordering\", \"ordering\", \"organise\", \"organise\", \"organise\", \"organise\", \"other\", \"other\", \"other\", \"other\", \"outstanding\", \"outstanding\", \"outstanding\", \"outstanding\", \"outstanding\", \"outstanding\", \"painless\", \"painless\", \"painless\", \"painless\", \"paperwork\", \"paperwork\", \"paperwork\", \"paperwork\", \"paperwork\", \"paperwork\", \"park\", \"park\", \"park\", \"park\", \"park\", \"pat\", \"pat\", \"pat\", \"patient\", \"patient\", \"patient\", \"patient\", \"pay\", \"pay\", \"pay\", \"pay\", \"pensby\", \"pensby\", \"pensby\", \"pensby\", \"people\", \"people\", \"people\", \"people\", \"people\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"personal\", \"personal\", \"personal\", \"personal\", \"personal\", \"phone\", \"phone\", \"phone\", \"photo\", \"photo\", \"photo\", \"photo\", \"pleasant\", \"pleasant\", \"pleasant\", \"pleasant\", \"pleasant\", \"pleasant\", \"pleased\", \"pleased\", \"pleased\", \"pleased\", \"pleasure\", \"pleasure\", \"pleasure\", \"pleasure\", \"pleasure\", \"plenty\", \"plenty\", \"plenty\", \"plenty\", \"point\", \"point\", \"point\", \"point\", \"point\", \"polite\", \"polite\", \"polite\", \"polite\", \"polite\", \"polite\", \"poor\", \"poor\", \"poor\", \"poor\", \"practical\", \"practical\", \"practical\", \"press\", \"press\", \"press\", \"price\", \"price\", \"price\", \"price\", \"pricessuper\", \"pricessuper\", \"pricessuper\", \"privately\", \"privately\", \"privately\", \"privately\", \"process\", \"process\", \"process\", \"process\", \"procure\", \"procure\", \"procure\", \"product\", \"product\", \"product\", \"product\", \"productsvery\", \"productsvery\", \"productsvery\", \"professional\", \"professional\", \"professional\", \"professional\", \"professionalism\", \"professionalism\", \"professionalism\", \"professionalism\", \"professionalism\", \"proficient\", \"proficient\", \"proficient\", \"proficient\", \"progress\", \"progress\", \"progress\", \"progress\", \"progress\", \"promise\", \"promise\", \"promise\", \"promise\", \"promise\", \"prompt\", \"prompt\", \"prompt\", \"prompt\", \"properly\", \"properly\", \"properly\", \"properly\", \"properly\", \"properly\", \"properly\", \"publicly\", \"publicly\", \"publicly\", \"publicly\", \"quality\", \"quality\", \"quality\", \"quick\", \"quick\", \"quick\", \"quick\", \"quick\", \"quid\", \"quid\", \"quid\", \"quid\", \"quid\", \"rain\", \"rain\", \"rain\", \"rain\", \"rain\", \"range\", \"range\", \"range\", \"range\", \"range\", \"rate\", \"rate\", \"rate\", \"rate\", \"really\", \"really\", \"really\", \"really\", \"reap\", \"reap\", \"reap\", \"reasonable\", \"reasonable\", \"reasonable\", \"reasonable\", \"reccomende\", \"reccomende\", \"reccomende\", \"receive\", \"receive\", \"receive\", \"receive\", \"recieve\", \"recieve\", \"recieve\", \"recieve\", \"recomend\", \"recomend\", \"recomend\", \"recomend\", \"recomend\", \"recommend\", \"recommend\", \"recommend\", \"recovered\", \"recovered\", \"recovered\", \"regard\", \"regard\", \"regard\", \"regard\", \"regard\", \"regard\", \"relate\", \"relate\", \"relate\", \"relate\", \"reliable\", \"reliable\", \"reliable\", \"remarkable\", \"remarkable\", \"remarkable\", \"remarkably\", \"remarkably\", \"remarkably\", \"remarkably\", \"reminder\", \"reminder\", \"reminder\", \"remove\", \"remove\", \"remove\", \"repeat\", \"repeat\", \"repeat\", \"repeat\", \"rescue\", \"rescue\", \"rescue\", \"reservation\", \"reservation\", \"reservation\", \"reservation\", \"resolve\", \"resolve\", \"resolve\", \"resolve\", \"resolve\", \"respond\", \"respond\", \"respond\", \"respond\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"revolutionise\", \"revolutionise\", \"revolutionise\", \"reward\", \"reward\", \"reward\", \"rim\", \"rim\", \"rim\", \"rim\", \"road\", \"road\", \"road\", \"road\", \"road\", \"road\", \"rude\", \"rude\", \"rude\", \"rummage\", \"rummage\", \"rummage\", \"rummage\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run_flat\", \"run_flat\", \"run_flat\", \"run_flat\", \"run_flat\", \"s\", \"s\", \"s\", \"safety\", \"safety\", \"safety\", \"safety\", \"safety\", \"safety\", \"same\", \"same\", \"same\", \"same\", \"satisfied\", \"satisfied\", \"satisfied\", \"satisfied\", \"save\", \"save\", \"save\", \"save\", \"saved\", \"saved\", \"saved\", \"saved\", \"savibg\", \"savibg\", \"savibg\", \"say\", \"say\", \"say\", \"say\", \"say\", \"score\", \"score\", \"score\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"seal\", \"seal\", \"seal\", \"seamless\", \"seamless\", \"seamless\", \"seamless\", \"seamless\", \"seamless\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"selection\", \"selection\", \"selection\", \"sensible\", \"sensible\", \"sensible\", \"sensible\", \"serious\", \"serious\", \"serious\", \"serious\", \"service\", \"service\", \"service\", \"service\", \"service\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"several\", \"short_notice\", \"short_notice\", \"short_notice\", \"shortage\", \"shortage\", \"shortage\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"significantly\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"site\", \"site\", \"site\", \"sized\", \"sized\", \"sized\", \"slick\", \"slick\", \"slick\", \"slot\", \"slot\", \"slot\", \"slow\", \"slow\", \"slow\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth_transaction\", \"smooth_transaction\", \"smooth_transaction\", \"smooth_transaction\", \"smoothly\", \"smoothly\", \"smoothly\", \"smoothly\", \"speedy\", \"speedy\", \"speedy\", \"speedy\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"staff\", \"staff\", \"staff\", \"staff\", \"stamp\", \"stamp\", \"stamp\", \"standard\", \"standard\", \"standard\", \"standard\", \"start\", \"start\", \"start\", \"start\", \"steering\", \"steering\", \"steering\", \"still\", \"still\", \"still\", \"still\", \"straight_away\", \"straight_away\", \"straight_away\", \"straight_away\", \"straightforward\", \"straightforward\", \"straightforward\", \"straightforward\", \"straightforward\", \"strand\", \"strand\", \"strand\", \"strand\", \"strand\", \"strand\", \"stuck\", \"stuck\", \"stuck\", \"suit\", \"suit\", \"suit\", \"support\", \"support\", \"support\", \"support\", \"support\", \"swan\", \"swan\", \"swan\", \"swift\", \"swift\", \"swift\", \"swift\", \"swift\", \"swore\", \"swore\", \"swore\", \"system\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"take\", \"team\", \"team\", \"team\", \"team\", \"technical\", \"technical\", \"technical\", \"technical\", \"technical\", \"technical\", \"tell\", \"tell\", \"tell\", \"tell\", \"terrific\", \"terrific\", \"terrific\", \"test\", \"test\", \"test\", \"test\", \"test\", \"texte\", \"texte\", \"texte\", \"texte\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"tho\", \"tho\", \"tho\", \"thoroughly\", \"thoroughly\", \"thoroughly\", \"tighten\", \"tighten\", \"tighten\", \"tighten\", \"time\", \"time\", \"time\", \"timing\", \"timing\", \"timing\", \"timing\", \"tire\", \"tire\", \"tire\", \"tire\", \"top\", \"top\", \"top\", \"top\", \"top\", \"torque\", \"torque\", \"torque\", \"total\", \"total\", \"total\", \"total\", \"total\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"trent\", \"trent\", \"trent\", \"turn\", \"turn\", \"turn\", \"turn\", \"turnaround\", \"turnaround\", \"turnaround\", \"tye\", \"tye\", \"tye\", \"tyer\", \"tyer\", \"tyer\", \"tyer\", \"tyre\", \"tyre\", \"tyre\", \"tyre\", \"tyresgreat\", \"tyresgreat\", \"tyresgreat\", \"ue\\u00ba\", \"ue\\u00ba\", \"ue\\u00ba\", \"unsure\", \"unsure\", \"unsure\", \"unsure\", \"use\", \"use\", \"use\", \"use\", \"use\", \"value\", \"value\", \"value\", \"value\", \"value\", \"valve\", \"valve\", \"valve\", \"valve_stem\", \"valve_stem\", \"valve_stem\", \"vast\", \"vast\", \"vast\", \"vast\", \"void\", \"void\", \"void\", \"void\", \"wait\", \"wait\", \"wait\", \"waiting_room\", \"waiting_room\", \"waiting_room\", \"waiting_room\", \"waiting_room\", \"waiting_room\", \"waiting_room\", \"walking\", \"walking\", \"walking\", \"wash\", \"wash\", \"wash\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"weather\", \"weather\", \"weather\", \"weather\", \"website\", \"website\", \"website\", \"week\", \"week\", \"week\", \"welcome\", \"welcome\", \"welcome\", \"welcome\", \"well\", \"well\", \"well\", \"well\", \"well\", \"whatsoever\", \"whatsoever\", \"whatsoever\", \"whatsoever\", \"wheel\", \"wheel\", \"wheel\", \"wheel\", \"wheel_nut\", \"wheel_nut\", \"wheel_nut\", \"wheel_nut\", \"whole\", \"whole\", \"whole\", \"whole\", \"whole\", \"whole\", \"wife\", \"wife\", \"wife\", \"wife\", \"work\", \"work\", \"work\", \"work\", \"worked_perfectly\", \"worked_perfectly\", \"worked_perfectly\", \"worked_perfectly\", \"would\", \"would\", \"would\", \"would\", \"woulnt\", \"woulnt\", \"woulnt\", \"woulnt\", \"write\", \"write\", \"write\", \"write\", \"wrong\", \"wrong\", \"wrong\", \"yet\", \"yet\", \"yet\", \"yet\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 10, 3, 8, 2, 1, 9, 5, 4, 7]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el8501401631716811049877980820\", ldavis_el8501401631716811049877980820_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el8501401631716811049877980820\", ldavis_el8501401631716811049877980820_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el8501401631716811049877980820\", ldavis_el8501401631716811049877980820_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "5     -0.253493 -0.132947       1        1  41.227957\n",
              "9     -0.359107  0.073339       2        1  40.266418\n",
              "2      0.019369  0.188030       3        1   4.973177\n",
              "7      0.089958 -0.036707       4        1   2.991416\n",
              "1      0.086166 -0.012440       5        1   2.392269\n",
              "0      0.085662 -0.015023       6        1   2.196544\n",
              "8      0.083559 -0.028421       7        1   1.884330\n",
              "4      0.085269 -0.014526       8        1   1.569551\n",
              "3      0.081696 -0.010574       9        1   1.329115\n",
              "6      0.080920 -0.010731      10        1   1.169223, topic_info=           Term         Freq        Total Category  logprob  loglift\n",
              "27        price  2776.000000  2776.000000  Default  30.0000  30.0000\n",
              "215       would   878.000000   878.000000  Default  29.0000  29.0000\n",
              "121   recommend   649.000000   649.000000  Default  28.0000  28.0000\n",
              "18      service  3715.000000  3715.000000  Default  27.0000  27.0000\n",
              "20         good  3004.000000  3004.000000  Default  26.0000  26.0000\n",
              "...         ...          ...          ...      ...      ...      ...\n",
              "731       ahead     0.957364    11.747612  Topic10  -7.1934   1.9416\n",
              "1045       wife     1.552613    31.803285  Topic10  -6.7099   1.4292\n",
              "984        vast     1.074922    32.233946  Topic10  -7.0776   1.0481\n",
              "1772       link     0.780089    12.360263  Topic10  -7.3982   1.6860\n",
              "464       point     0.803150    69.134953  Topic10  -7.3691  -0.0064\n",
              "\n",
              "[450 rows x 6 columns], token_table=      Topic      Freq      Term\n",
              "term                           \n",
              "1295      1  0.215306  addition\n",
              "1295      2  0.143537  addition\n",
              "1295      3  0.071769  addition\n",
              "1295      4  0.071769  addition\n",
              "1295      5  0.287075  addition\n",
              "...     ...       ...       ...\n",
              "344       3  0.007234     wrong\n",
              "483       1  0.719855       yet\n",
              "483       2  0.098162       yet\n",
              "483       3  0.032721       yet\n",
              "483       6  0.065441       yet\n",
              "\n",
              "[1595 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[6, 10, 3, 8, 2, 1, 9, 5, 4, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXCYmnz48XR6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}