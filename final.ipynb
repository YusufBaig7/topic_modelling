{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in e:\\program_files\\conda\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: Cython==0.29.21 in e:\\program_files\\conda\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in e:\\program_files\\conda\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in e:\\program_files\\conda\\lib\\site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in e:\\program_files\\conda\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: nltk in e:\\program_files\\conda\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in e:\\program_files\\conda\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in e:\\program_files\\conda\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: joblib in e:\\program_files\\conda\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: tqdm in e:\\program_files\\conda\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: numpy in e:\\program_files\\conda\\lib\\site-packages (1.19.5)\n",
      "Requirement already satisfied: pandas in e:\\program_files\\conda\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in e:\\program_files\\conda\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in e:\\program_files\\conda\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in e:\\program_files\\conda\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\program_files\\conda\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "ERROR: No matching distribution found for re\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in e:\\program_files\\conda\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (4.47.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (8.0.7)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: setuptools in e:\\program_files\\conda\\lib\\site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: jinja2 in e:\\program_files\\conda\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in e:\\program_files\\conda\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in e:\\program_files\\conda\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in e:\\program_files\\conda\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\program_files\\conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in e:\\program_files\\conda\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\program_files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program_files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in e:\\program_files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in e:\\program_files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\program_files\\conda\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: sklearn in e:\\program_files\\conda\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in e:\\program_files\\conda\\lib\\site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\program_files\\conda\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in e:\\program_files\\conda\\lib\\site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in e:\\program_files\\conda\\lib\\site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\program_files\\conda\\lib\\site-packages (from scikit-learn->sklearn) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install re\n",
    "!pip install spacy\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tidytext\n",
      "  Using cached tidytext-0.0.1.tar.gz (4.3 kB)\n",
      "Collecting siuba\n",
      "  Using cached siuba-0.0.25-py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: pandas>=0.24.0 in e:\\program_files\\conda\\lib\\site-packages (from siuba->tidytext) (1.0.5)\n",
      "Requirement already satisfied: PyYAML>=3.0.0 in e:\\program_files\\conda\\lib\\site-packages (from siuba->tidytext) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in e:\\program_files\\conda\\lib\\site-packages (from siuba->tidytext) (1.19.5)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.19 in e:\\program_files\\conda\\lib\\site-packages (from siuba->tidytext) (1.3.18)\n",
      "Requirement already satisfied: pytz>=2017.2 in e:\\program_files\\conda\\lib\\site-packages (from pandas>=0.24.0->siuba->tidytext) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in e:\\program_files\\conda\\lib\\site-packages (from pandas>=0.24.0->siuba->tidytext) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\program_files\\conda\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.24.0->siuba->tidytext) (1.15.0)\n",
      "Building wheels for collected packages: tidytext\n",
      "  Building wheel for tidytext (setup.py): started\n",
      "  Building wheel for tidytext (setup.py): finished with status 'done'\n",
      "  Created wheel for tidytext: filename=tidytext-0.0.1-py3-none-any.whl size=3905 sha256=048321031fa96879618963c3ea1fe8494c855b7c04c3cddbfd30c2ab28a023c4\n",
      "  Stored in directory: c:\\users\\yusuf\\appdata\\local\\pip\\cache\\wheels\\07\\03\\c0\\f73eeef462dd66dbca0288a338fcbcdc78e3588937ccc907d8\n",
      "Successfully built tidytext\n",
      "Installing collected packages: siuba, tidytext\n",
      "Successfully installed siuba-0.0.25 tidytext-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tidytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program_files\\Conda\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import guidedlda\n",
    "import sklearn\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import wordnet\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tires where delivered to the garage of my choi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Easy Tyre Selection Process, Competitive Prici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very easy to use and good value for money.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Really easy and convenient to arrange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was so easy to select tyre sizes and arrang...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Tires where delivered to the garage of my choi...\n",
       "1  Easy Tyre Selection Process, Competitive Prici...\n",
       "2         Very easy to use and good value for money.\n",
       "3              Really easy and convenient to arrange\n",
       "4  It was so easy to select tyre sizes and arrang..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.read_csv('data.csv')\n",
    "# Print head\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    tires where delivered to the garage of my choi...\n",
       "1    easy tyre selection process  competitive prici...\n",
       "2           very easy to use and good value for money \n",
       "3                really easy and convenient to arrange\n",
       "4    it was so easy to select tyre sizes and arrang...\n",
       "Name: paper_text_processed, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = papers['text'].map(lambda x: re.sub('[,\\.!?]', ' ', x))\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x))\n",
    "# Convert the titles to lowercase\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers['paper_text_processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tires', 'where', 'delivered', 'to', 'the', 'garage', 'of', 'my', 'choice', 'the', 'garage', 'notified', 'me', 'when', 'they', 'had', 'been', 'delivered', 'day', 'and', 'time', 'was', 'arranged', 'with', 'the', 'garage', 'and', 'went', 'and', 'had']\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-sm==3.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl#egg=en_core_web_sm==3.1.0 in e:\\program_files\\conda\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in e:\\program_files\\conda\\lib\\site-packages (from en-core-web-sm==3.1.0) (3.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.47.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: jinja2 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: setuptools in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in e:\\program_files\\conda\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in e:\\program_files\\conda\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in e:\\program_files\\conda\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: six in e:\\program_files\\conda\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\program_files\\conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in e:\\program_files\\conda\\lib\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in e:\\program_files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\program_files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in e:\\program_files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\program_files\\conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in e:\\program_files\\conda\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-25 21:51:44.758085: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-07-25 21:51:44.758486: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tire', 'deliver', 'garage', 'choice', 'garage', 'notify', 'deliver', 'day', 'time', 'arrange', 'garage', 'go', 'fit', 'hassel', 'free', 'experience']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 3), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "dictionary = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 arrange\n",
      "1 choice\n",
      "2 day\n",
      "3 deliver\n",
      "4 experience\n",
      "5 fit\n",
      "6 free\n",
      "7 garage\n",
      "8 go\n",
      "9 hassel\n",
      "10 notify\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3f38eea1dc5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbow_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessed_docs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "#bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=10, id2word=dictionary, passes=10, workers=2, alpha='symmetric',eta=0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    '''tags parts of speech to tokens\n",
    "    Expects a string and outputs the string and \n",
    "    its part of speech'''\n",
    "    \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "def word_lemmatizer(text):\n",
    "    '''lemamtizes the tokens based on their part of speech'''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = lemmatizer.lemmatize(text, get_wordnet_pos(text))\n",
    "    return text\n",
    "def reflection_tokenizer(text):\n",
    "    '''expects a string an returns a list of lemmatized tokens \n",
    "    and removes the stop words. Tokens are lower cased and \n",
    "    non- alphanumeric characters as well as numbers removed. '''\n",
    "    text=re.sub(r'[\\W_]+', ' ', text) #keeps alphanumeric characters\n",
    "    text=re.sub(r'\\d+', '', text) #removes numbers\n",
    "    text = text.lower()\n",
    "    tokens = [word for word in word_tokenize(text)]\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "    #removes smaller than 3 character\n",
    "    tokens = [word_lemmatizer(w) for w in tokens]\n",
    "    tokens = [s for s in tokens if s not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['lemmatize_token'] = papers.text.apply(reflection_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>paper_text_processed</th>\n",
       "      <th>lemmatize_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tires where delivered to the garage of my choi...</td>\n",
       "      <td>tires where delivered to the garage of my choi...</td>\n",
       "      <td>[tire, deliver, garage, choice, garage, notify...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Easy Tyre Selection Process, Competitive Prici...</td>\n",
       "      <td>easy tyre selection process  competitive prici...</td>\n",
       "      <td>[easy, tyre, selection, process, competitive, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very easy to use and good value for money.</td>\n",
       "      <td>very easy to use and good value for money</td>\n",
       "      <td>[easy, use, good, value, money]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Really easy and convenient to arrange</td>\n",
       "      <td>really easy and convenient to arrange</td>\n",
       "      <td>[really, easy, convenient, arrange]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was so easy to select tyre sizes and arrang...</td>\n",
       "      <td>it was so easy to select tyre sizes and arrang...</td>\n",
       "      <td>[easy, select, tyre, size, arrange, local, fit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Tires where delivered to the garage of my choi...   \n",
       "1  Easy Tyre Selection Process, Competitive Prici...   \n",
       "2         Very easy to use and good value for money.   \n",
       "3              Really easy and convenient to arrange   \n",
       "4  It was so easy to select tyre sizes and arrang...   \n",
       "\n",
       "                                paper_text_processed  \\\n",
       "0  tires where delivered to the garage of my choi...   \n",
       "1  easy tyre selection process  competitive prici...   \n",
       "2         very easy to use and good value for money    \n",
       "3              really easy and convenient to arrange   \n",
       "4  it was so easy to select tyre sizes and arrang...   \n",
       "\n",
       "                                     lemmatize_token  \n",
       "0  [tire, deliver, garage, choice, garage, notify...  \n",
       "1  [easy, tyre, selection, process, competitive, ...  \n",
       "2                    [easy, use, good, value, money]  \n",
       "3                [really, easy, convenient, arrange]  \n",
       "4  [easy, select, tyre, size, arrange, local, fit...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.to_csv('data_lemmatized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectorizer = CountVectorizer(tokenizer = reflection_tokenizer, min_df=10, stop_words=stoplist, ngram_range=(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program_files\\Conda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "X_ngrams = token_vectorizer.fit_transform(papers.paper_text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10132, 2570)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ngrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = token_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = dict((v, idx) for idx, v in enumerate(tf_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list_6 = [['cost', 'effective', 'expensive', 'money', 'value', 'price', 'cheap','good price', 'reasonable','pricing'],\n",
    "                   ['quick', 'efficient', 'great service', 'service','fast', 'good service', 'excellent', 'professional', 'rude', 'bad','service'],\n",
    "                   ['easy','appointment', 'book','schedule'],\n",
    "                   ['tyre', 'great', 'finish', 'good', 'awesome', 'excellent', 'fitting', 'reliable'],\n",
    "                   ['fitter', 'mobile', 'mechanic', 'staff', 'friendly','polite'],\n",
    "                   ['area', 'near', 'far','location'],\n",
    "                   ['wait', 'hour', 'time','fast' ],\n",
    "                   ['deliver', 'order','timely'],\n",
    "                   ['confusion', 'rebook'],\n",
    "                   ['wait','long', 'waste', 'time'],\n",
    "                   ['discount', 'offer', 'price'],\n",
    "                   ['reschedule', 'rebook', 'change', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dict = {'Value for Money' : 0, 'garage service' : 1, 'ease of booking' : 2 , 'tyre quality' : 3, 'mobile fitter' : 4,\n",
    "             'location' : 5, 'length of fitting' : 6, 'delivery punctuality' : 7, 'booking confusion' : 8, 'wait time' : 9, 'discounts' :\n",
    "              10, 'change of date': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"cost\"] : seed_dict[\"Value for Money\"], word2id[\"effective\"] : seed_dict[\"Value for Money\"], \n",
    "    word2id[\"expensive\"] : seed_dict[\"Value for Money\"], word2id[\"money\"] : seed_dict[\"Value for Money\"], \n",
    "    word2id[\"value\"] : seed_dict[\"Value for Money\"], word2id[\"price\"] : seed_dict[\"Value for Money\"], \n",
    "    word2id[\"cheap\"] : seed_dict[\"Value for Money\"], word2id[\"good price\"] : seed_dict[\"Value for Money\"],\n",
    "    word2id[\"reasonable\"] : seed_dict[\"Value for Money\"], word2id[\"pricing\"] : seed_dict[\"Value for Money\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"quick\"] : seed_dict[\"garage service\"], word2id[\"efficient\"] : seed_dict[\"garage service\"], \n",
    "    word2id[\"great service\"] : seed_dict[\"garage service\"], word2id[\"service\"] : seed_dict[\"garage service\"], \n",
    "    word2id[\"fast\"] : seed_dict[\"garage service\"], word2id[\"good service\"] : seed_dict[\"garage service\"], \n",
    "    word2id[\"excellent\"] : seed_dict[\"garage service\"], word2id[\"professional\"] : seed_dict[\"garage service\"],\n",
    "    word2id[\"rude\"] : seed_dict[\"garage service\"], word2id[\"bad\"] : seed_dict[\"garage service\"], \n",
    "    word2id[\"service\"] : seed_dict[\"garage service\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"easy\"] : seed_dict[\"ease of booking\"], word2id[\"appointment\"] : seed_dict[\"ease of booking\"], \n",
    "    word2id[\"book\"] : seed_dict[\"ease of booking\"], word2id[\"schedule\"] : seed_dict[\"ease of booking\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"tyre\"] : seed_dict[\"tyre quality\"], word2id[\"great\"] : seed_dict[\"tyre quality\"], \n",
    "    word2id[\"finish\"] : seed_dict[\"tyre quality\"], word2id[\"good\"] : seed_dict[\"tyre quality\"],\n",
    "    word2id[\"awesome\"] : seed_dict[\"tyre quality\"], word2id[\"excellent\"] : seed_dict[\"tyre quality\"],\n",
    "    word2id[\"fitting\"] : seed_dict[\"tyre quality\"], word2id[\"reliable\"] : seed_dict[\"tyre quality\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"fitter\"] : seed_dict[\"mobile fitter\"], word2id[\"mobile\"] : seed_dict[\"mobile fitter\"], \n",
    "    word2id[\"mechanic\"] : seed_dict[\"mobile fitter\"], word2id[\"staff\"] : seed_dict[\"mobile fitter\"],\n",
    "    word2id[\"friendly\"] : seed_dict[\"mobile fitter\"], word2id[\"polite\"] : seed_dict[\"mobile fitter\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"area\"] : seed_dict[\"location\"], word2id[\"near\"] : seed_dict[\"location\"], \n",
    "    word2id[\"far\"] : seed_dict[\"location\"], word2id[\"location\"] : seed_dict[\"location\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"wait\"] : seed_dict[\"length of fitting\"], word2id[\"hour\"] : seed_dict[\"length of fitting\"], \n",
    "    word2id[\"time\"] : seed_dict[\"length of fitting\"], word2id[\"fast\"] : seed_dict[\"length of fitting\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"deliver\"] : seed_dict[\"delivery punctuality\"], word2id[\"order\"] : seed_dict[\"delivery punctuality\"], \n",
    "    word2id[\"timely\"] : seed_dict[\"delivery punctuality\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"confusion\"] : seed_dict[\"booking confusion\"], word2id[\"rebook\"] : seed_dict[\"booking confusion\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"wait\"] : seed_dict[\"wait time\"], word2id[\"long\"] : seed_dict[\"wait time\"], \n",
    "    word2id[\"time\"] : seed_dict[\"wait time\"], word2id[\"waste\"] : seed_dict[\"wait time\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"discount\"] : seed_dict[\"discounts\"], word2id[\"offer\"] : seed_dict[\"discounts\"], \n",
    "    word2id[\"price\"] : seed_dict[\"discounts\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics.update({\n",
    "    word2id[\"reschedule\"] : seed_dict[\"change of date\"], word2id[\"rebook\"] : seed_dict[\"change of date\"], \n",
    "    word2id[\"change\"] : seed_dict[\"change of date\"], word2id[\"date\"] : seed_dict[\"change of date\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidedlda import GuidedLDA as lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 10132\n",
      "INFO:guidedlda:vocab_size: 2570\n",
      "INFO:guidedlda:n_words: 163366\n",
      "INFO:guidedlda:n_topics: 12\n",
      "INFO:guidedlda:n_iter: 1000\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -1860300\n",
      "INFO:guidedlda:<50> log likelihood: -1119680\n",
      "INFO:guidedlda:<100> log likelihood: -1101654\n",
      "INFO:guidedlda:<150> log likelihood: -1094332\n",
      "INFO:guidedlda:<200> log likelihood: -1090766\n",
      "INFO:guidedlda:<250> log likelihood: -1088768\n",
      "INFO:guidedlda:<300> log likelihood: -1087419\n",
      "INFO:guidedlda:<350> log likelihood: -1085991\n",
      "INFO:guidedlda:<400> log likelihood: -1085749\n",
      "INFO:guidedlda:<450> log likelihood: -1085369\n",
      "INFO:guidedlda:<500> log likelihood: -1084287\n",
      "INFO:guidedlda:<550> log likelihood: -1084744\n",
      "INFO:guidedlda:<600> log likelihood: -1083924\n",
      "INFO:guidedlda:<650> log likelihood: -1083784\n",
      "INFO:guidedlda:<700> log likelihood: -1083454\n",
      "INFO:guidedlda:<750> log likelihood: -1082671\n",
      "INFO:guidedlda:<800> log likelihood: -1082780\n",
      "INFO:guidedlda:<850> log likelihood: -1082146\n",
      "INFO:guidedlda:<900> log likelihood: -1082922\n",
      "INFO:guidedlda:<950> log likelihood: -1081665\n",
      "INFO:guidedlda:<999> log likelihood: -1082643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: value good service easy good value money great value money great value quick efficient excellent use tyre way simple buy save quick easy excellent value good value money way buy effective great service easy use\n",
      "Topic 1: service great price great service excellent great price efficient customer excellent service service great customer service fast friendly competitive use service great price efficient service quick easy price great good service easy always great service great fantastic\n",
      "Topic 2: easy use price tyre website easy use good choice fitting great book garage local use website easy use website excellent site competitive selection choose range price easy fitter competitive price website easy\n",
      "Topic 3: price good service tyre excellent great fitting good price choice excellent service quality great price garage tyre good fit choice tyre good choice price excellent excellent price centre tyre great price tyre quality tyre price good great service\n",
      "Topic 4: tyre fit car garage wheel get new fitter take fitting service good new tyre one order back go would come tyre fit use change job time issue\n",
      "Topic 5: best price use best price easy tyre convenient fitting time always start finish service start finish could find station good fitting station deal cheapest simple could find never like\n",
      "Topic 6: use service time recommend definitely would first excellent definitely use time use tyre price class first class would definitely good always friend first time definitely recommend second experience excellent service second time first time use\n",
      "Topic 7: order easy tyre fit garage time fitting easy order price process book local online good deliver local garage tyre fit great order tyre simple straight quick forward get straight forward\n",
      "Topic 8: service recommend friendly would excellent highly tyre highly recommend staff helpful professional free hassle use garage great quick efficient hassle free easy excellent service would recommend fit great service experience\n",
      "Topic 9: tyre time day garage book fitting appointment fit get email service order say would arrive customer date told call change deliver wait car could customer service\n",
      "Topic 10: good price service good price easy good service quick service good local great use garage price good fitting efficient service good price delivery great price fitter easy use price good service fast arrange local garage good price good\n",
      "Topic 11: tyre price fit garage get local easy fitting use want cheaper well go choose time great way tyre fit save get tyre else buying local garage tyre want buy\n"
     ]
    }
   ],
   "source": [
    "model = lda(n_topics= 12, n_iter=1000, random_state=1, refresh=50)\n",
    "model.fit(X_ngrams, seed_topics = seed_topics, seed_confidence = 0.3)\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 25\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "     topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "     print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.41420214e-05 1.35995332e-05 2.60199922e-03 ... 1.49376327e-01\n",
      "  8.78460807e-05 5.69230088e-02]\n",
      " [7.60890350e-04 1.22685140e-03 4.76738801e-01 ... 1.18087222e-04\n",
      "  8.63175408e-04 4.02596157e-04]\n",
      " [9.92441354e-01 1.47019383e-04 2.41009353e-03 ... 3.14606630e-05\n",
      "  3.32795266e-03 1.60815234e-04]\n",
      " ...\n",
      " [1.13717283e-04 3.90809695e-05 2.79728421e-04 ... 5.17186447e-03\n",
      "  1.68268351e-01 3.89153637e-01]\n",
      " [4.33170017e-04 6.34722554e-01 6.34345550e-04 ... 3.45378364e-03\n",
      "  8.92604203e-04 7.40833658e-04]\n",
      " [6.05681489e-06 1.46229230e-04 9.76687633e-04 ... 8.29760797e-03\n",
      "  3.06448589e-03 8.97674623e-04]]\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.transform(X_ngrams)\n",
    "print(doc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.60890350e-04, 1.22685140e-03, 4.76738801e-01, 3.99776131e-01,\n",
       "       1.27955238e-04, 5.31552156e-04, 3.19315036e-04, 1.18882062e-01,\n",
       "       2.52582438e-04, 1.18087222e-04, 8.63175408e-04, 4.02596157e-04])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "      <th>topic 10</th>\n",
       "      <th>topic 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic 0  topic 1  topic 2  topic 3  topic 4  topic 5  topic 6  topic 7  \\\n",
       "0     0.00     0.00     0.00     0.00     0.01     0.00      0.0     0.75   \n",
       "1     0.00     0.00     0.48     0.40     0.00     0.00      0.0     0.12   \n",
       "2     0.99     0.00     0.00     0.00     0.00     0.00      0.0     0.00   \n",
       "3     0.52     0.00     0.00     0.00     0.00     0.02      0.0     0.45   \n",
       "4     0.00     0.00     0.88     0.00     0.00     0.00      0.0     0.11   \n",
       "5     0.00     0.32     0.00     0.00     0.01     0.00      0.0     0.00   \n",
       "6     0.00     0.29     0.38     0.30     0.00     0.00      0.0     0.00   \n",
       "7     0.00     0.00     0.00     0.99     0.00     0.00      0.0     0.00   \n",
       "8     0.00     0.01     0.10     0.00     0.01     0.00      0.0     0.02   \n",
       "9     0.00     0.00     0.13     0.00     0.00     0.00      0.0     0.00   \n",
       "\n",
       "   topic 8  topic 9  topic 10  topic 11  \n",
       "0     0.03     0.15      0.00      0.06  \n",
       "1     0.00     0.00      0.00      0.00  \n",
       "2     0.00     0.00      0.00      0.00  \n",
       "3     0.00     0.00      0.00      0.00  \n",
       "4     0.00     0.00      0.00      0.00  \n",
       "5     0.00     0.65      0.00      0.00  \n",
       "6     0.00     0.00      0.02      0.00  \n",
       "7     0.00     0.00      0.00      0.00  \n",
       "8     0.00     0.00      0.02      0.83  \n",
       "9     0.00     0.00      0.85      0.00  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_label = ['topic {}'.format(i) for i in range(12)]  # number of topics\n",
    "topic_vector = pd.DataFrame(doc_topic, columns = columns_label)#dataframe of doc-topics\n",
    "topic_vector.round(2).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_threshold(doc_topic, topic_vector):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the topic number if the probablity of a topic being in a document is more than value\n",
    "    \"\"\"\n",
    "    \n",
    "    topic_num_list = []\n",
    "    for i in range(len(topic_vector)):\n",
    "        topic_num = [idx for idx, value in enumerate(doc_topic[i]) if value >= 0.25]\n",
    "        if topic_num != []:\n",
    "            topic_num = topic_num\n",
    "        else:\n",
    "            topic_num = 'None'\n",
    "        topic_num_list.append(topic_num)\n",
    "    return topic_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topic=topic_threshold(doc_topic, topic_vector)\n",
    "#print(num_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_topic = pd.DataFrame({'topics': num_topic, 'reflection': papers.paper_text_processed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_topic.to_csv(\"ded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
